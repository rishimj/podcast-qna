# Podcast Transcript
# Show: Latent Space: The AI Engineer Podcast
# Episode: Scaling Test Time Compute to Multi-Agent Civilizations â€” Noam Brown, OpenAI
# YouTube URL: https://www.youtube.com/watch?v=ddd4xjuJTyg
# Downloaded: 2025-07-19 20:58:27

Hey everyone, welcome to the L and Space
podcast. This is Allesio, partner and
CTO of Deible and I'm joined by my
co-host Spooks, founder of Small AI.
Hello. Hello. And we are here recording
on a holiday Monday with Nan Brown from
OpenAI. Welcome. Thank you. So glad to
have you finally join us. Uh, a lot a
lot of people have heard you. You've
been rather generous of your time on on
like podcasts. Um, Lex Friedman and
you've done a you've done a TED talk
recently just talking about the thinking
paradigm, but I think maybe perhaps your
most interesting recent achievement is
winning the world diplomacy
championship. Yeah. In 2022, you you
built like sort of Cicero which was top
10% of human players. I guess my opening
question is how has your diplomacy
playing changed since working on Cicero
and then now personally playing it? When
you work on these games, you kind of
have to understand the game well enough
to like be able to debug your bot
because if the bot does something that's
like really radical and like that t
humans typically wouldn't do, you're not
sure if that's like a mistake or if
that's just uh like if it's a bug in the
system or it's actually just like the
bot being brilliant. When we were
working on Diplomacy, I kind of like did
this deep dive like trying to understand
the game better. I played in in
tournaments. I like watched a lot of
like tutorial videos and commentary
videos on games and over that process I
got better. And then also seeing the bot
like the way it would behave in these
games like sometimes it would do things
that humans typically wouldn't do. And
that taught me about the game as well.
When we released Cicero, we announced it
in like late 2022. I still found the
game like really fascinating. And so I
like kept up with it. I like continue to
play and that led to me winning the
championship in the world championship
in 2025. So just a couple months ago.
There's always a question of like
centaur systems where humans and
machines work together. Like was there
an equivalent of what happened in Go
where you updated your play style
because you're asking if I used Cicero
when I played in the tournament. The
answer is the answer is no. Seeing the
way the bot played and like taking
inspiration from that I think did help
me in in the tournament. Yeah. Yeah. Do
people now ask turing questions every
single time when they're playing
diplomacy? ask to try to figure out if
uh if the person they're playing with is
a bot or Yeah, like that's the one thing
you're worried about when you started.
It was really interesting when we were
working on Cisero because like you know
we didn't have the best language models.
We were really bottlenecked on the
quality of the language models and
sometimes the bot would do would say
like bizarre things like you know 90 99%
of the time it was fine but then like
every once in a while it would say this
like really bizarre thing like it would
just hallucinate about something.
Somebody would reference something that
they said earlier in a conversation with
the bot and the bot would be like, "I
have no idea we're talking about. I
never said that." And then the person
would be like, "Look, you could just
scroll up in the chat." It's like
literally right there. And the bot would
be like, "No, you're Windows."
And when it does these kinds of things,
like people just kind of like shrugged
it off as like, "Oh, that's just, you
know, the person's tired or they're
drunk or whatever or they're just like
trolling me." But I think like that's
because people weren't looking for a
bot. They weren't expecting a bot to be
in the games. We're actually really
scared because we were afraid that
people would would figure out at one
point that that there's a bot in these
games and then they would just like
always be on the lookout for it and they
would always be and if you're if you're
looking for it, you're able to spot it.
That's the thing. So, I think now that
it's announced and that people know to
look for it, I think they would have an
easier time spotting it. Now, that said,
the language models have also gotten a
lot better since 2022. It's adversarial.
Yeah. So, at this point, like, you know,
the truth is, you know, GP40 and like
03, these models are like passing the
touring test. So I don't think they can
really ask that many touring complete
questions that would actually make a
difference. And Cesar was very small
like 2.7b, right? It was a very small uh
language model. Yeah. This is one of the
things we realized over the course of
the project that like oh yeah you you
really benefit a lot from just having
like larger language models, right? Yep.
How do you think about today's
perception of AI and a lot of like maybe
the safety discourse of like you know
you're kind of built a bot that is
really good at persuading people into
like helping them win a game and I think
maybe today labs want to say they don't
work on that type of problem. How do you
think about that dichotomy so to speak
between the two? No, honestly like after
we released Cicero, a lot of the AI
safety community was really happy with
the research and like the way it worked
because it was a very controllable
system like we conditioned Cicero on
certain concrete actions and that gave
it a lot of steerability to say like
okay well it's going to pursue a
behavior that we can like very clearly
interpret and and very clearly define.
It's not just like oh it's a language
model like running loose and doing
whatever it feels like. It's actually
like pretty steerable and there's this
whole reasoning system that steers the
way the language model interacts with
the human. Actually, a lot of
researchers reached out to like reached
out to me and said like we think this is
like potentially a really good way to
achieve safety with these systems. I
guess the last diplomacy related
questions that we might have is have you
updated or tested like Oer models on
diplomacy and would you would you expect
a lot more difference? I have not. I
think I said this on on Twitter at one
point that I think this would be a great
benchmark. I would love to see all the
leading bots play a game of diplomacy
with each other and see like who does
best and I think a couple people have
like taken inspiration from that and are
actually like building out these
benchmarks and like evaling the models.
My understanding is that they don't do
very well right now. Um but I think I
think it really is a fascinating
benchmark and I think it would be um
yeah, I think it'd be a really cool
thing to to try out. Well, we're going
to go a little bit into O series now. I
think the last time you did a lot of
publicity, you were just launching 01.
You did your TED talk and everything.
How has the vibe, how have the vibes
changed just in general? You said you
were very excited to learn from domain
experts like in chemistry like how they
review the old series models like how
have you updated since let's say end of
last year? I think the trajectory was
pretty clear pretty early on in the
development cycle. And I think that
everything that's unfolded since then
has been pretty on track for what I
expected. So, I wouldn't say that my
perception of where where things are
going or has honestly changed that much.
Um, I think that we're going to continue
to see I I said before that we're going
to see this paradigm continue to
progress rapidly and I think that that's
true even today that we we saw that with
like going from 01 preview to 01 to 03
consistent progress and we're going to
continue to see that going forward and I
think that we're going to see a
broadening of what these models can do
as well. You know, like we're going to
start seeing agentic behavior. We're
already starting to see agentic
behavior. Like honestly for me 03 I've
been using it a ton in my day-to-day
life. I just find it so useful this
especially the fact that I can now
browse the web and like you know do
meaningful research on my behalf like
it's kind of like a mini deep research
that you can just get a response in
three minutes. So yeah I think it's just
going to continue to become more and
more useful and uh more powerful as time
goes on and pretty quickly. Yeah. And
talking about deep research you tweeted
about if you need proof that we can do
this in number of viable domains. Deep
research is kind of like a great
example. Can you maybe talk about if
there's something that people are
missing? You know I feel like I hear
that repeated a lot. It's like you know
it's easier to do encoding and math but
like not in these other domains. I
frequently get this question including
from pretty established AI researchers
that okay we're seeing these like
reasoning models succeed in math and
coding and these these easily verifiable
domains but are they ever going to
succeed in domains where success is less
well defined? I'm surprised that this is
such a common perception because we've
released deep research and people can
try it out. People do use it. It's very
popular and that is very clearly a
domain where you don't have an easily
verifiable metric for success. It's very
like what what is the best research
report that you could generate and yet
these models are doing extremely well at
this at this domain. So I think that's
like an existence proof that these
models can succeed in tasks that don't
have as easily verifiable rewards. Is it
because there's also not necessarily
like a wrong answer like there's a
spectrum of deep research quality,
right? you can have like a report that
like looks good but the information is
kind of so and so and then you have a
great report. Do you think people have a
hard time understanding the difference
when they get the result? My impression
is that people do understand the
difference when they get a result and
and I think that they're surprised at
how good the deep research results are.
There's certainly it's not not 100%. It
could be better and we're going to make
it better, but I think people can tell
the difference between a good report and
a bad report and and certainly and a
good report and a mediocre report and
that's enough to kind of feed the the
loop later to build the product and
improve the model performance. I mean, I
think if you're in a situation where
people can't tell the difference between
the outputs, then it doesn't really
matter if you're like, you know, hill
climbing on on progress. Uh these models
are going to get better at domains where
there is a measure of success. Now I
think this idea that it has to be like
easily verifiable or something like
that. I don't think that's true. I think
that you can have you can have these
models do well even in domains where
success is a very difficult to define
thing could sometimes even be
subjective. people lean on a lot you've
you've done as well is the thinking fast
as slow analogy for just uh thinking
models and I think it's reasonably well
diffused now the idea of uh that that
this is kind of the next scaling
paradigm all analogies are imperfect
what is one way in which thinking fast
and slow or system one system two kind
of doesn't transfer to how we actually
scale these things one thing that I
think is underappreciated is that the
models the pre-trained models need a
certain level of capability in order to
really benefit from this like extra
thinking. This is kind of why you you've
seen the reasoning paradigm emerge
around the time that it did. I think it
could have happened earlier, but if you
try to do the reasoning paradigm on top
of GBD2, I don't think it would have
gotten you almost anything. Is this
emergence? Hard to say I if it's
emergence necessarily, but like I
haven't done the um you know the
measurements to really define that
clearly. Um, but I think it's pretty
clear, you know, people tried chain of
thought with GBD, like really small
models, and they saw that it just didn't
really do anything. Then you go to
bigger models and it starts to to give a
lift. I think there's a lot of debate
about like the extent to which this kind
of behavior is emergent, but clearly
there is a difference. So, it's not like
there are these two independent
paradigms. I think that they are related
in the sense that you need a certain
level of system one capability in your
models in order to have system two is to
be able to benefit from system two.
Yeah, I have play tried to play amateur
neuroscientists before and try to
compare it to the evolution of the brain
and how you have to evolve the cortex
first before you evolve the other parts
of the brain. And perhaps that is what
we're doing here. Yeah. And you could
argue that actually this is not that
different from like I guess the um the
system one system two paradigm because
you know if you ask like a pigeon to
think really hard about playing chess
you know it's not going to get that far.
You know it doesn't matter if it like
thinks for a thousand years it's like
not going to be able to be better at
playing chess. So maybe you do still
also also in like with animals and
humans that you need a certain level of
intellectual ability just in terms of
system one in order to benefit from
system two as well. Yeah. Just this t
side tangent does this also apply to
visual reasoning. So let's say we have
now we have the 40 like natively
omnimodel type of thing then that also
makes 03 really good at geogesser. Does
that apply to other modalities too? I I
think the evidence is yes. It depends on
exactly the kinds of questions that
you're asking. Like there are some
questions that I think don't really
benefit from system 2. I think geo
guests are certainly one uh where where
you do benefit. I think image
recognition if if I had to guess it's
like one of those things where you
probably benefit less from system 2
thinking cuz you know it or you don't.
Yeah. Exactly. There's no way. Yeah. And
the the thing the thing I typically
point to is just like information like
retrieval. If somebody asks you like
when was this person born and you don't
have access to the web, then you either
know it or you don't and you can sit
there and you can think about it for a
long time. Maybe you can make an
educated guess and you can say like well
this person was like probably lived
around this time and so this is like a
rough date. But you're not going to be
able to like get the date unless you
actually just just know it. But like
spatial reasoning like tic-tac-toe might
be better because you have all the
information there. Yeah. And I think
it's true that like with tic-tac-toe we
see that like GPD 4.5 falls over. You
know, it plays decently well. I
shouldn't say it falls over. It does
reasonably well. You can draw the board.
It can make legal moves, but it will
make mistakes sometimes. And if you
really need that system too to enable it
to play perfectly. Now, it's possible
that if you got to GBD6 and you just did
system one, it would also play
perfectly. You know, I guess we'll we'll
know one day, but I think right now you
would need the system two to really like
do well. What do you think are like the
things that you need in system one? So
obviously general understanding of like
game rules. Do you also need to
understand some sort of like metag game
of like you know usually this is like
how you value pieces in different games
even though it's a you know how do you
generalize in system one so that then in
system two you can kind of get to the
gameplay so to speak. I think the more
that you have in your in the system one,
like this is the same thing with humans,
you know, like humans are when they're
playing for the first time, uh, a game
like chess, they can apply a lot of
system two thinking to it. And if you if
you apply a ton of system two thinking
to it, like if if you just present a
really smart person with a completely
novel game, and you tell them like,
okay, you're going to play this game
against like an AI or like a human
that's like mastered this game, and you
tell them to like sit there and and
think about it for like 3 weeks about
how to play this game. My guess is they
could actually do pretty well, but it
certainly helps to build up that system
one thinking like build up intuition
about about the game because it will
just make you so much Yeah. so much
faster. I think the Pokemon example is a
good one of like the system one kind of
has maybe all this information about
games and then once you put it in the
game it still needs a lot of harnesses
to work and I'm trying to figure out how
much of can we take things from the
harness and have them in system one to
the system two as harness free as
possible but I guess that's like the
question about generalizing games and AI
yeah I guess I view that as a different
question I think the question about like
harnesses in my view is that the ideal
harness is no harness
Yeah, right. I think harnesses are like
a crutch that eventually we're going to
be able to move beyond. So, only two
costs and you could ask, you know, you
could just ask O3. And actually, you
know, it's interesting cuz like when
this uh playing Pokemon thing kind of
like emerged as as this like, you know,
benchmark. I was actually like pretty
opposed to evaling this with our with
our like open eye models because my
feeling is like, okay, if we're going to
do this eval, let's just do it with 03.
you know, how far does 03 get without
any harness? How far does it get playing
PokÃ©mon? And the answer is like not very
far, you know? Um, and that's fine. I
think it's fine to have an eval where
the models do terribly. And I don't
think the answer to that should be like,
well, let's build a really good harness
so that now it can do well on this eval.
I think the answer is like, okay, well,
let's just like improve the capabilities
of our models so they can do well at
everything and then they also happen to
make progress on this eval. Would you
consider things like checking for a
valid move a harness or is this in the
in the model? You know, like chess. It's
like you can either have the model learn
in system one what moves are valid and
what it can and cannot do versus in
system two figuring out. I think I think
there's like a lot of this is design
questions. Like for me, I think you
should give the model the ability to
check if a move is legal if you want.
like that that could be an option in the
environment of like okay here's a you
know an action that you can like a tool
call that you can make to see if an
action is legal if it wants to use that
it it can and then there's like design
question of like well what do you do if
the model makes an illegal move and I
think it's totally reasonable to say
like well if they make an illegal move
then they lose the game like I don't
know what what happens when a human
makes an illegal move in a game of chess
I actually don't know they're just not
allowed to yeah like do you just lose
the game I don't know so if that's if
that's the case then I think it's
totally reasonable to say like yeah
we're going to have an eval where that's
also the criteria for for the AI models.
Yeah. But I think like maybe one way to
interpret that in sort of researcher
terms is are you allowed to do search?
And one of the famous findings from Deep
Seek is that MCTS wasn't that useful to
them. But I think like there are a lot
of engineers trying out search and
spending a lot of tokens doing that and
maybe it's not worth it. Well, I'm
making a distinction here between like a
tool call to check whether a move is
legal or illegal is different from
actually making that move and then
seeing whether it ended up being legal
or illegal. Right? So, if that tool call
is available, I think it's totally fine
to make that tool call and check whether
a move is legal or illegal. I think it's
different to have the model say, "Oh,
I'm making this move." Yeah. And then,
you know, it gets feedback that like,
"Oh, you made an illegal move." And so,
then it's like, "Oh, just kidding. Like,
I'm going to do something else now." So,
so that's that's the distinction I'm I'm
drawing. Some people have tried to
classify that second type of laying
things out as test time compute. You
would not classify that as test time
compute. There's a lot of reasons why
you would not want to rely on that
paradigm when you're going to the
imagine you have a robot, you know, and
your robot like takes some action in the
world and it like breaks something and
you're just like, oh, you can't say
like, oh, just kidding. I didn't mean to
do that. I'm going to undo that action.
Like the thing is broken. So if you want
to simulate what would happen if I move
the robot in this way and then in your
simulation you saw that this thing broke
and then you decide not to do that
action that's totally fine but you can't
just like undo actions that you've taken
in the world. There's a couple more
things I wanted to cover in this rough
area. I actually had an answer on the on
the thinking fast and slow side which
maybe I I'm curious what you think about
like a lot of people are trying to put
in effectively model router layers let's
say between like the the fast response
model and the the long thinking model
enthropic is explicitly doing that and I
think there's a question about always do
you need a smart judge to route or do
you need a dumb judge judge to route
because it's fast so when you have a
model router let's say let's say you're
passing requests between system one side
and system two
Does the router need to be as smart as
the smart model or dumb to be fast? I
think it's possible for a dumb model to
recognize that a problem is really hard
and that it won't be able to solve it
and then route it to a a more capable
model, but it's also possible for a dumb
model to be fooled or to be
overconfident. I don't know. I think
there's a real trade-off there. But I
will say like I think I think there are
a lot of things that people are building
right now that will eventually be washed
away by scale. So I think harnesses are
a good example where I think eventually
the models are going to be and I think
this actually happened with the
reasoning models like before the
reasoning models emerged there was like
all of this work that went into
engineering these like agentic systems
that like made a lot of calls to GBD40
or like the these non-reasoning models
to get reasoning behavior and then it
turns out like oh we just like created
reasoning models and they you don't need
this like complex behavior. In fact, in
many ways it makes it worse like you
just give the reasoning model the same
question without any sort of scaffolding
and it just does it now that you can
still and so people are building
scaffolding on top of the reasoning
models right now but I think in many
ways like those scaffolds will also just
be replaced by the reasoning models and
models in general becoming more capable
and similarly I think things like model
uh like these routers you know we've
said pretty openly that we want to move
to a world where there is a single
unified model and in that world you
shouldn't need a router on top of the
model. So I think that the router issue
will eventually be solved. Also like
you're building the router into the
model kind of weights itself. I don't
think there will be a a a benefit for
like I I shouldn't say because it's I
could be wrong about this like you know
and certainly maybe there's um you know
reasons to route to different model
providers or whatever but I think that
routers are going to um eventually go
away and I can understand why it's worth
doing it in the short term because like
the fact is it is beneficial right now
and if you're building a product and you
you're getting a lift from it then it's
it's worth doing right now. One of the
tricky things I' I'd imagine that a lot
of developers are facing is that like
you kind of have to plan for where these
models are going to be in six months and
12 months and that's like very hard to
do because things are progressing very
quickly. You know, you don't want to
spend 6 months building something and
then just have it be totally washed away
by scale. But I I think I would
encourage developers like when when
they're, you know, building these kinds
of things like scaffolds and and
routers, keep in mind that the field is
evolving very rapidly. You know, things
are going to change in 3 months, uh, let
alone 6 months. And that might require
radically changing these things around
or or tossing them out completely. So
don't spend 6 months building something
that might get tossed out in 6 months.
It's so hard though. Everyone says this
and then like no one has concrete
suggestions on how.
What about reinforcement fine-tuning? Is
this something that obviously you just
released it a month ago at Openai. Is
that something people should spend time
on right now or maybe wait until the
next jump? I think reinforcement fine is
is pretty cool and I I think it's like
worth looking into because it's really
about specializing the models for the
data that you have and I think that um
something that's like worth worth
looking into for for developers like
we're not we're not suddenly going to
like have that data baked into the raw
model a lot of times. So I I think
that's kind of like a separate question.
Yeah. So creating the environment and
the reward model is the best thing
people can do right now. I think the
question that people have is like should
I rush to fine-tune the the model using
RFT or should I build the harness to
then RFT the models as they get better.
I think the difference is that like for
reinforcement fine-tuning you're
collecting data that's going to be
useful as the models improve as well. So
if we come out with like future models
that are even more capable, you could
still fine-tune them on your data.
That's I think actually a good example
where you're building something that's
going to complement the model scaling
and becoming more capable rather than
necessarily getting washed away by the
scale. Y one last question on Ilia. You
mentioned on I think the Sarah and Elad
podcast where you had this conversation
with Ilia a few years ago about more RL
and reasoning and language models. just
any speculation or thoughts on why his
attempt when he tried it, it didn't work
or the timing wasn't right and why the
time is right now. I don't think I I
would frame it that way that like this
his attempt didn't work in many ways it
did. Um so Ilia for me I saw that in all
of these domains that I had worked on in
poker and hannabi and diplomacy having
the models think before acting made a
huge difference in performance like
orders of magnitude difference like
10,000 times or something. Yeah. Like
you know a thousand to 100,000 times
like it's the the equivalent of a model
that's like a thousand to 100,000 times
bigger. And in language models you
weren't really seeing that that the
model the models would just respond
instantly. Some people in the field in
the LLM field were like convinced that
like, okay, we just keep scaling
pre-training, we're going to get to
super intelligence. And I was kind of
skeptical of that perspective. In late
2021, I was having a meal with Ilia. He
asked me what my AGI timelines are. Very
standard SF question. And I told him
like, look, I think it's actually quite
far away because we're going to need to
figure out this reasoning paradigm in a
very general way. And with things like
LM, LM are very general, but they don't
have a reasoning paradigm that's very
general. And until they do, they're
going to be limited in what they can do.
Like, we're going to scale it. Sure,
we're going to scale these things up by
a few more orders of magnitude. They're
going to become more capable, but we're
not going to see super intelligence from
just that. And like, yes, if we had a
quadrillion dollars to train these
models, then maybe we would, but like
you're going to hit the limits of what's
economically feasible before you get to
super intelligence, unless you have a
reasoning paradigm. And I was convinced
incorrectly that the reasoning paradigm
would take a long time to figure out
because it's like this big unanswered
research question and you know Ilia
agreed with me and he said like yeah you
know I think we need this like
additional paradigm but his take was
that like maybe it's not that hard. I I
didn't know it at the time but like he
and others at OpenAI had also been
thinking about this. They had also been
thinking about RL. They had been working
on it and I think they had some success
but like you know with most research
like it does you have to iterate on
things. You have to try out different
ideas. you have to yeah try different
things and then also as the models
become more capable as they become
faster it becomes easier to iterate on
experiments and I think that the work
that they did even though it didn't like
result in a reasoning paradigm it all
builds on top of previous work right so
they built a lot of things that over
over time led to this reasoning paradigm
for listeners gnome can talk about this
but the rumor is that that thing was
cenamed GPT0 if you want to search for
that that line of work. I think there
was a time where like basically Aro kind
of went through a dark age when everyone
like went all in on it and then nothing
happens and they gave up and like now
it's like kind of the golden age again.
So that's what I'm like trying to
identify like why what is it and it
could just be that we have smarter base
models and better data. I don't think
it's just that we have smarter base
models. I I think it's that yeah so I we
did end up getting a big success with
with reasoning and I but I think it was
in many ways a gradual thing to some
extent it was gradual you know like
there were signs there were signs of
life and then we like you know iterated
and tried out some more things we got
like better signs of life I think it was
around like no november 2023 or October
2023 when I think I was convinced that
we had like very conclusive signs of
life that like oh this this was going to
be this is the paradigm and it's going
to be a big
that was in many ways a a gradual thing.
I think what OpenAI did well is like
when we got those signs of life, they
recognized it for what it was and
invested heavily in in scaling it up.
And I think that's that's ultimately
what what led to reasoning models
arriving when they did. Was there any
disagreement internally especially
because like you know OpenAI kind of
pioneer pre-training scaling, you know,
and kind of like computers all you need
and then you're kind of saying maybe
that's not how we get there. Was it
clear to everybody that like, okay, this
is going to work or was it
controversial? There's always different
opinions about this stuff. I think there
were some people that felt that
pre-training was all we need and we
scaled it up to infinity and we were
there. I think a lot of the leadership
actually at OpenAI recognized that there
was another paradigm that was needed and
that was why they were investing all
this like research effort into this like
RL um stuff. And I think that's also to
the credit of opening eye that like okay
yes they figured out the pre-training
paradigm and they were very focused on
scaling that up. In fact, the vast
majority of resources were focused on
scaling that up. But they also
recognized the value that that something
else was going to be needed and it was
worth researching putting researcher
effort into into other directions to
figure out what that extra paradigm was
going to be. There was a lot of debate
about first of all like what is that
extra paradigm. So I think a lot of the
researchers looked at reasoning and and
RL was not really about scaling test
time compute. It was more about data
efficiency because you know the feeling
was that well we have tons and tons of
compute but we actually are more limited
by data. So there's there's the data
wall and we're going to hit that before
we hit limits on on the compute. So how
do we make these algorithms more data
efficient? They are more data efficient
but I think that also like um they are
also just like the equivalent of scaling
up compute also by a ton. That was
interesting. There's like a lot of
debate around like okay what what
exactly are we doing here? And then I
think also even when we got the signs of
life I think there was a lot of debate
about the significance of it that like
okay how much should we invest in
scaling up this paradigm. I think
especially when you're when you're in a
small company like you know open AI like
in 2023 was not as big as it is today
and compute was more constrained than it
is today. And if you're investing
resources in in a direction that's
coming at the expense of something else.
And so if you look at these signs of
life on reasoning and you're saying
like, "Okay, well this looks promising.
We're going to scale this up by a ton
and invest a lot more resources into it,
where are those resources coming from?"
You have to make that tough call about
where to where to draw the resources
from. And that is a very controversial,
very difficult call to make um that
makes some people unhappy. And I think
there was debate about whether we're
focusing too much on this paradigm,
whether it's really a big deal, whether
we would see it generalize and do
various things. And I remember it was
interesting that I I talked to somebody
who left OpenAI after we had discovered
the reasoning paradigm but before we
announced 01 and they ended up going to
a computing lab. I saw them afterwards
after we announced um 01 and they told
me that like at the time they really
didn't think this like reasoning thing
like this these O series the strawberry
models were like that that big of a
deal. It was like they thought we were
making a bigger deal of it than it
really deserved to be. And then when we
announced 01 and they saw the reaction
of their co-workers at this competing
lab about how everybody was like, "Oh
crap, like this is a big deal." And they
like pivoted the whole research agenda.
Oh my god. To focus on this that then
they realized like, "Oh, actually like
this maybe is a big deal." You know, a
lot of this seems obvious in retrospect,
but at the time it's actually not so
obvious and be quite difficult to
recognize something for what it is. I
mean, OpenAI has like a great history of
just making the right bet. I feel GBD
models are kind of similar, right? Where
like it started with games and RL and
then it's like maybe we can just scale
these language models instead. And um
I'm just impressed by the leadership and
obviously the the research team that
keeps coming out with these insights.
Looking back on it today, it it might
seem obvious that like, oh, of course,
like these models get better with
scaled, so you should just scale them up
a ton and it'll get better. But it it
really is the best research is obvious
in retrospect and at the time it's it's
not as obvious as it might seem today.
Follow questions on data efficiency. Uh
this is this is a pet topic of mine. It
seems that our current methods of
learning are so inefficient still right
like compared to the existence proof of
humans. We take five samples and we
learn learn something. Machines 200
maybe you know per per like whatever
data point you might need. anyone doing
anything interesting in data efficiency
or do you think like there's just a
fundamental inefficiency that machine
learning has that will just always be
there compared to humans? I think it's a
good point that if you look at the
amount of data these models are trained
on and you compare it to like the amount
of data that a human observes to get the
same performance. I guess pre-training
it's a little hard to make an apples
apples comparison because like I don't
know how much how many tokens does a
baby actually absorb when they're
developing. But I think it's a fair
statement to say that these models are
are less data efficient than humans. And
I think that that's an unsolved research
question and probably one of the most
important unsolved research questions.
Maybe more important than algorithmic
improvements cuz you can just you can we
we can increase the supply of data out
of the existing set of the worlds and
humans. I guess okay. So a couple
thoughts on that. Like one is that the
answer might be an algorithmic
improvement. Like maybe maybe
algorithmic improvements do lead to
greater data efficiency. And the second
thing is that like it's not like humans
learn from just reading the internet. So
um I think it's certainly easiest to
learn from just like data that's on the
internet. Um but I don't think that's
like the limit of what data you could
collect. The last followup before we
change topics to coding. any other just
anecdotes or insights from Ilia just in
general cuz like you've worked with him
so there's not that many people that we
can talk to that have worked with him. I
think I've just been very very impressed
with his vision that I think like
especially when I joined and and I saw
you know the internal documents at at
OpenAI of like what he had been thinking
about back in like 2021 2022 even
earlier I I was very impressed that he
had a clear vision of like where this
was all going and what was needed. some
of his emails from 201617 when they were
founding open AI was published and even
then he was talking about how he thinks
like one big experiment is much more
valuable than 100 small ones that was
like a core insight that differentiated
them from brain for example. It just
seems very insightful that he just sees
things much more clearly than others and
I I just wonder what his production
function is like how do you make a human
like that and how do you improve your
own thinking to better model it. I I
mean I think it is true that I mean one
of openi's big success was betting on
the scaling paradigm. It is just kind of
odd because you know they were not the
biggest lab you know it was like
difficult for them to scale back then it
was much more common to do like a lot of
small experiments more academic style
people were trying to figure out these
um various like algorithmic improvements
and openi bet pretty early on like large
scale. We had David Wan on who I think
was VPenge at the time of GPT1 and 2 and
he talked about how the differences
between Brain and OpenAI was basically
the cause of the Google's inability to
come out with a scaled model like just
structurally everyone had allocated
compute and you had to pull resources
together to make bets and you just
couldn't. I think that's true that
OpenAI was structured differently and I
think that really helped them like
OpenAI functions a lot like a startup
and other places tended to function more
like universities or or you know
research labs as they traditionally
existed. the way that OpenAI operates
more like as a startup with this mission
of building AGI and and super
intelligence that helped them organize,
collaborate, pull resources together,
make hard choices about like how to
allocate resources and I think a lot of
the other labs like have now been trying
to adopt paradigms more like that like
setups more like that. Let's talk about
maybe the killer use case at least in my
mind of these models which is coding.
Yeah, you released codeex recently, but
I would love to talk through the gnome
brown coding stack. What models you use,
how you interact with them, cursor, wind
surf. Uh lately I've been using windsurf
and codeex like actually a lot of
codecs. I've been having a lot of fun.
Like you just give it a task and it just
goes off and does it and comes back five
minutes later with like a you know pull
request. And is it core research task or
like side stuff that you don't super
care about? I wouldn't say it's like
side stuff. I would say basically
anything that I would normally try to
code up, I try to do it with codeex
first. Well, for you it's free, but
yeah, for everybody it's free right now.
And I think that's partly because it's
the it's the most effective way for me
to do it and also it's good for me to
get experience working with this
technology and then also like seeing the
shortcomings of it. It just helps me
like better understand like okay this is
the the limits of these models and like
what we need to push on next. Have you
felt the AGI? I felt the AJ multiple
times. Yes.
Like like um how should people push
codeex in ways that you've you've done
and you know I think you you see it
before others cuz obviously you were
closer to it. I think anybody can use
codecs and feel the AGI. It's kind of
funny how like you feel the AGI and then
you get used to it very quickly, you
know? So So it's really like
dissatisfied with like where it's
lacking. Yeah, I know. And you know it's
it's magical one day. I was actually
looking back at the old uh Sora videos
when they were announced cuz like you
remember when Sora came out it was just
like the biggest news ever. It was just
magical. You look at that and you're
like it's like it's really here like
this is AGI. But you look at it now and
it's kind of like oh you know the people
like don't move like very organically
and it's like there's like a lack of
consistency in some ways and you see all
these flaws in it now that you just
didn't really notice when it was first
came out and yeah you get used to this
technology very quickly and but I think
what's cool about it is that because
it's developing so quickly you get those
feely AGI moments like every few months.
So something else is going to come out
and just like it's magical to you and uh
and then you get used to it very
quickly. Yeah. What are your winds surf
pro tips now that you've immersed in it?
I think one thing I'm surprised by is
how few people I mean maybe your
audience is going to be more comfortable
with reasoning models and like use
reasoning models more. But I'm surprised
at how many people don't even know that
03 exists. Like I've been using it dayto
day. It's basically replaced Google
search for me. Like I just use it all
the time. like and also for things like
coding like I I tend to just use the
reasoning models. My suggestion is like
if people are not have not tried the
reasoning models yet because like
honestly like we do like people love
them people that use it love them
obviously a lot more people use GBD40 um
and just like the default and what on
chatbt and that kind of stuff I think
it's worth trying the reasoning models
like I think people would be surprised
at what they can do. I use Windsurf
daily and they still haven't actually
enabled it as like a default in
Windsurf. Like I always have to dig up
like type in 03 and then it then it's
like oh yeah that that exists. It's it's
uh it's weird. I would say like my
struggle with it has been that it's
takes so long to reason and actually
break out of flow. I think that is true.
Yes. And and I think this this is one of
the advantages of Codeex that like okay
you can give it a task that's kind of
self-contained and like it can go off
and do its thing and come back 10
minutes later. And I can see that if
you're doing if you're using this thing
as like more like a like a pair
programmer kind of thing, then yeah, you
want to use GP4.1 or something like
that. What do you think are the most
broken part of the development cycle
with AI? Like in my mind, it's like um
pull request review. Like for me, like I
use codecs all the time and then I got
all these pull requests and it's kind of
hard to like go through all of them.
What other thing would you like people
to build to make this CVN more scalable?
I think it's really on us to build a lot
more stuff. These models are very
limited in in in some ways. I think I
find it frustrating that, you know, you
ask them to do something and they spend
10 minutes doing it and then you ask
them to do something pretty similar and
then they go spend 10 minutes doing it
and like, you know, it's I I think I
describe them as like they're geniuses,
but it's their first day on the job, you
know, and that's like kind of annoying.
Like even the the smartest person on
earth when they're when it's their first
day on the job, you know, they're not
going to be like as useful as you would
like them to be. So I think being able
to get more experience and like act like
somebody that's actually been on the job
for like 6 months instead of what one
day I think would make them a lot more
useful but that's really on us to build
to build that capability. Do you think a
lot of it is like GPU constraint for
you? Like if I think about codeex why is
it asking me to set up the environment
myself when like the model if I ask code
three to like create an environment
setup script for a repo I'm sure it'll
be able to do it but today in the
product I have to do it. So I'm
wondering in your mind, could these be a
lot more if we just again put more test
time compute on them or do you think
there's like a fundamental model
capability limitation today that we
still need a lot of like human harnesses
around it? I think that we're in an
awkward state right now where like
progress is very fast and there's things
that are like clearly we could do this
and the models would be better. We're
going to get to it. It's um you're just
limited by how many hours there are in
the day, you know? So progress can only
proceed so quickly. We're trying to get
to everything as fast as we can and and
I think that the 03 is not where the
technology will be in six months. I like
that question overall in like there's a
software development life cycle not just
generation of the code like from issue
to PR basically is is like the the
typical commentary of that and then
there's the winds surf side which is
insider your ID like what else right
pull request review is like something
that people don't really there are
startups that built around it it's not
something that codeex does and it could
and so like then there's like what else
is there you know that is sort of rate
limiting the amount of software you
could be iterating on it's an open
question. I don't I don't I don't know
if there's an answer. Anything else on
on Ace in general? Like where do you
think this goes just in form factors or
what will we be looking at this time
next year in terms of how things are how
what models were able to do that they're
not able to today? I don't think it's
going to be limited to ASU, you know. I
think I don't think it's going to be
limited to software engineering. I think
it's going to be able to do a lot of
remote work kind of tasks. Yeah. Like
freelancer type Upwork. Yeah. Yeah. Or
just like even things that are not
necessarily software engineering. Okay.
So, the way that I think about it is
like anybody that's doing a remote work
kind of job, I think it's valuable to
become familiar with the technology and
like kind of get a sense of like what it
can do, what it can't do, what it's good
at, what it's not good at because I
think the the breadth of things that
it's going to be able to do is going to
expand over time as well. I feel like
virtual assistants might be the next
thing after as then because they're the
most easily like you know virtual assist
like hire someone in the Philippines
someone uh who who just look through
your email and all that because that is
entirely you can intercept all the
inputs and all the outputs and train on
that and maybe open just buys a virtual
assistant company. Yeah, I think what
I'm looking forward to is that um for
things like virtual assistants, the the
models like if they're aligned well,
they could end up being like really
preferable for that for that kind of
work. You know, if there's always this
like principal agent problem where if
you delegate a task to somebody, then
like are they really aligned with like
doing it as you would want it to be done
and u just as cheaply as quickly as they
can. Yeah. Yeah. And so if you have an
AI model that's like actually really
aligned to you and your preferences,
then that could end up doing a way
better job than a human could. Well, not
not it's doing a better job than a human
could, but like it's doing a better job
than a human would. That word alignment,
by the way, I think there's like an
interesting overwriting or uh homorphism
between safety alignment and instruction
following alignment. And I wonder where
they diverge. Okay, so I think where it
diverges is like what do you want to
align the models to like that? That's I
think a difficult question, you know,
like you could say like you wanted to
align it to the user. Okay. Well, what
happens if the user wants to build a
novel virus that's going to wipe out
half of humanity? Safety alignment. So
there's a question of like I think
alignment I think they're related, you
know, and I think that the big question
is like what are you aligning towards?
Yeah. There's like humanity goals and
then there's your personal goals and
everything in between. Mhm. So that's
kind of I guess the individual agent and
you announced the you're leading the
multi- aent team at OpenAI. I haven't
really seen many announcements. Maybe I
missed them on what you've been working
on, but what can you share about
interesting research directions or um
anything from there? Yeah, there's uh
hasn't really been announcements on
this. I think we're working on cool
stuff and I think we'll get to announce
some cool stuff uh at some point. I
think the team in many ways is actually
a misnomer because we're working on more
more than just multi-agent. Multi-agent
is one of the things we're working on.
Um some other things we're working on is
just like being able to scale up test
time compute by by a ton. So how you
know we get these models thinking for 15
minutes now how do we get them to think
for hours how do we get them to think
for days even longer and be able to
solve incredibly difficult problems so
that's one direction that we're pursuing
uh multi-agent is another direction and
here I think there's a few different
motivations uh we're interested in like
both the collaborative and the
competitive aspect of multi- aent I
think the way that I describe it is
people often say in AI circles that
humans occupy this very narrow band of
intelligence and AI are just going to
like quickly catch up and then surpass
like this band of intelligence. And I
actually don't think that the band of
int of human intelligence is that
narrow. I think it's actually quite
broad because if you compare
anatomically identical humans from, you
know, caveman times, they didn't get
that far in terms of like, you know,
what we would consider intelligence
today, right? They're not putting a man
on the moon, you know, they're not like
building semiconductors or nuclear
reactors or anything like that. And and
we have those today even though we as
humans are not anatomically different.
And so what's the difference? Well, I
think the difference is that you have
thousands of years, a lot of humans,
billions of humans cooperating and
competing with each other, building up
civilization over time. And the
technology that we're seeing is the
product of this civilization. And I
think similarly, the AIs that we have
today are kind of like the cavemen of
AI. And and I think that if you're able
to um have them cooperate and compete
with billions of AIs over a long period
of time and build up a civilization,
essentially the things that they would
be able to produce and answer would be
far beyond what is possible today with
with the AS that we have today. Do you
see that being similar to maybe like Jim
Fan's Voyager skill library idea
re-saving these things or is it just the
models then being retrained on this new
knowledge because the humans then have
it a lot of it in the brain as they
grow? I think I'm going to be evasive
here and say that like we're we're not
going to yeah we're not going to
we're until we have something to
announce which I think that I think that
we will in the not too distant future. I
think I'm going to uh be a bit vague
about like exactly what we're doing. But
I will say that the way that we are
approaching multi- aent in the details
and the way we're actually going about
it is I think very different from how
it's been done historically and how it's
being done today by by other places. Um
I've been in the multi- aent field for a
long time. I've kind of felt like the
multi- aent field has been a bit
misguided in some ways and the things
that the approaches that the field has
taken and like the way it's been
approached. And so I think we're trying
to take a very principled approach to
multi- aent. Sorry, I got to ask like so
you you can't talk about what you're
doing, but you can say what's misguided.
What's misguided? I think that a lot of
the approaches that have been taken have
been very heristic and haven't really
been following like the bitter lesson
approach to scaling and research. Okay,
I think maybe this might be a good a
good spot. So obviously you've done a
lot of amazing work in in poker and I
think as the recent model got better I
was talking to one of my friends who
used to be a a hardcore poker grinder
and I told them I was going to interview
you and uh their question was at the
table you can get a lot of information
from a small sample size about how a
person plays but today GTO is like so
prevalent that sometimes people forget
that you can play exploitatively what do
you think is the state as you think
about multi-agent and kind of like
competition is it always going to be
trying to find the optimal thing or is a
lot of it trying to think more in the
moment like how to exploit somebody. I'm
guessing your audience is probably not
super familiar with poker terminology.
So, I'll just like explain this a bit.
Uh,
a lot of people think that poker is just
like a luck game and that's not true.
It's actually there's a lot of strategy
in poker. So, you can win consistently
in poker if you're playing the right
strategy. So, there's different
approaches to poker. One is game theory
optimal. This is like you're playing an
unbeatable strategy and expectation.
Like you're just unexploitable. It's
kind of like in rock paper scissors. You
can be unbeatable in rock paper scissors
if you just randomly choose between rock
paper and scissors with equal
probability because no matter what the
other guy does, you know, they're not
going to be able to exploit you or
you're going to win. You're going to
like not lose an expectation. Now, a lot
of people hear that and they think like,
well, that also means that you're not
going to win an expectation because
you're just playing totally randomly.
But in poker, if you play the
equilibrium strategy, it's actually
really difficult for the opponents to
figure out how to tie you, and they're
going to end up making mistakes that
will lead you to win over the long run.
It might not be a massive win, but it is
going to be a win. If you play enough
hands for a long enough period of time,
you're you're going to win in
expectation. Now, there's also
exploitative poker, and the idea here is
that you're trying to spot weaknesses in
how the opponent plays. you know, maybe
they're maybe they're not bluffing
enough or maybe they fold too easily to
a bluff. And so you start adapting from
the game theory optimal balance strategy
of like you bluff sometimes, you you
don't bluff sometimes to then playing a
very unbalanced strategy that's like,
oh, I'm just going to like bluff a ton
against this person because they always
fold whenever I bluff. Now, the key is
that there's a trade-off here because if
you're taking this exploitative
approach, then you're opening yourself
up to exploitation as well. And so you
have to choose this balance between
playing a defensive game theory optimal
policy that guarantees you're not going
to lose but might not make you as much
money as you potentially could versus
playing an exploitative strategy that
could be much more profitable but also
it creates weaknesses that the opponents
could take advantage of and trick you.
And there's no way to perfectly balance
the two. It's kind of like in rock paper
scissors if you notice somebody is like
playing paper for five times in a row
you might think like oh they're they
they have a weakness in their strategy.
I should just be throwing scissors and
I'm going to take advantage of them. And
so on the sixth time you throw scissors,
but actually that's the time when they
throw rock, you know, so and you never
really know. So you always have this
trade-off. The poker AIs that have been
extremely successful and like my
background is like I worked on AI for
poker for several years during grad
school and made the first superhuman no
limit poker AIs, the approach that we
took was this game theory optimal
approach where the AIs would play this
unbeatable strategy and they would play
against the world's best and beat them.
Now that also means they they beat the
world's worst. Like they would just beat
anybody. But if they were up against a
weak opponent, they might not beat them
as severely as a human expert might
because the human expert would know how
to adapt from the game optimal policy to
be able to exploit these weak players.
And so there's this kind of unanswered
question of like how do you make an
exploitative poker AI? And a lot of
people had pursued this research
direction. I like dabbled in it a little
bit during grad school. And I think
fundamentally it just comes down to AI
not being as sample efficient as humans.
You know, we discussed earlier, if a
human's playing poker, they're able to
get a really good sense of of the
strengths and weaknesses of a player
within a dozen hands. It's like honestly
really impressive. And back when we were
working on AI for poker in like the, you
know, mid2010s, you'd have to these AIs
would have to play like 10,000 hands of
poker to like get a good profile of like
who this player is, like how they're
playing, where their weaknesses are.
Now, I think with more recent
technology, that has come down. Um, but
still the sample efficiency has been a
big challenge. Now, what's interesting
is that after working on poker, I worked
on diplomacy. I think we talked about
this earlier. And diplomacy is this, you
know, it's a seven player negotiation
game. And when we started working on it,
I took a very game theory approach to
the problem. I I felt like, okay, we're
it's kind of like poker. You have to
compute this game theory optimal policy,
and you just play this, you're going to
not lose an expectation. You're going to
win in practice. But that actually
doesn't work in diplomacy. And it
doesn't work. Again, for question of
like how how much of a rabbit hole do we
want to go down on this, but like
basically when you're playing like the
zero sum games like like poker, game
theory optimal works really well. When
you're playing a game like Diplomacy
where there's like you need to
collaborate and compete and you need
there's there's room for collaboration,
then game theory optimal actually
doesn't work that well and you have to
understand the players and adapt to them
much better. So, this ends up being very
similar to the problem in poker of like
how do you adapt to your opponents. In
poker, it's about adapting to their
weaknesses and take advantage of that.
In diplomacy, it's about adapting to
their play styles. It's kind of like if
you're at a table and everybody's
speaking French, you don't want to just
keep talking in English. You want to
adapt to them and speak in French as
well. That's the realization that I have
with diplomacy that we need to shift
away from this game theory optimal
paradigm towards modeling the other
players, understanding who they are, and
then responding accordingly. And so in
many ways the techniques that we
developed in diplomacy are exploitative
like they're not exploitative. They're
they're really, you know, just adapting
to the to the opponents to the other
players at at the table. Um but I think
the same techniques could be used in AI
for poker to make exploitative poker
eyes. If I didn't get, you know, AGI
pill by the incredible progress that we
were seeing with language models and
like shifting my whole research agenda
to focusing on like general reasoning,
probably what I would have worked on
next was making these like exploitative
poker AI. It would be a really fun
research direction to go down. I think
it's still there for anybody that wants
to do it. And I think the key would be
taking the techniques that we used in in
diplomacy and applying them to things
like poker. I think to me the core piece
is when you play online you have a HUD
which tells you you know all these stats
about the other player like you know how
much they participate pre flop blah blah
blah and to me it's like a lot of these
models from my understanding are not
really leveraging the behavior of the
other players at the table they're just
kind of looking at the board state and
kind of working from there that's
correct the way the the way the poker
eyes work today they're just kind of
like sticking to their precomputed GTO
GTO strategy and they're not adapting to
the other players um at the table and
like you can do various like kind of
hacky things to get them to adapt but
you know they're not they're not very
principled they're not they don't work
super well y okay any grad students
listening uh if you want to work on that
I I think that is a very very reasonable
research direction that'll at least uh
get in front of you and you know get
some attention at least the other thing
that this conversation brings up for me
is yeah well one of the hypothesis for
like what is the next step after testime
compute is world models is world
modeling
importance or worthwhile research
direction like Yan Lakun has been
talking about this non-stop but like
basically no LLM have like they have
internal world models but like not
explicitly a world model I think it's
pretty clear that as these models get
bigger they have a world model and that
world model becomes better uh with scale
so they are implicitly developing a
world model and I don't think it's
something that you need to explicitly
model. Um, I could be wrong about that.
You know, there's when when dealing with
people or multi- aents, it might be
because you have entities that are not
the world and you're resolving
hypotheses of what which of the many
types of entities you could be dealing
with. You know, there was this like long
debate in the multi-agent AI community
for a long time about and it's still
going on about whether you need to
explicitly model other agents like other
like other people or if they can be
implicitly modeled this part of the
environment. For a long time, I was like
on the on the took the perspective of
like of course you have to like
explicitly model these other agents
because they're they're behaving
differently from the environment. Like
they they take actions, they're
unpredictable, you know, they they have
agency. But I think I've actually
shifted over time to thinking that like
actually if these models become smart
enough, they develop things like theory
of mind. They develop an understanding
that there are other agents that like
can take actions and and have motives
and all this stuff. And these models
just develop that implicitly with scale
and and more capable behavior broadly.
So that's the perspective I take these
days. So like what I just said was an
example of a heristic that is not bitter
lesson filled and you just it just goes
away. Yeah. It's really all come back to
the bitter lesson. got to cite them
every every podcast. So, one of the
interesting findings and most consistent
findings, you know, I think you were at
ICLR and uh one of the hit talks there
was about open-endedness and this guy
Tim who gave that talk has been doing a
lot bunch of research about multi- aent
systems too. One of the most consistent
findings is always that um it's better
for AIS to selfplay and improve
competitively as opposed to sort of
humans training and guiding them. And
you find that with like you know alpha
zero and R10 whatever that was. Do you
think this will hold for multi- aents
like selfplay to improve better than
humans? Yeah. So okay so this is a great
question and I I think this is like
worth expanding on. So I think a lot of
people today see selfplay as like the
next step and maybe the last step that
we need for super intelligence. And I
think if you're following, you know, you
look at something like Alpha Alpha Go
and Alpha Zero, we seem to be following
a very similar trend, right? Like the
first step in Alpha Go was you do large
scale pre-training. In that case, it was
on human Go games. With LMS, it's
pre-training on, you know, tons of like
internet data and that gets you a strong
model, but it doesn't get you uh you
know an extremely strong model, you
know, it doesn't get you superhuman
model. And then the next step in the
alpha go paradigm is you do large scale
test time compute or like large scale
inference compute and in that case with
um MCTS and now we have like reasoning
models that also do like this large
scale inference compute and again that
like boosts the capabilities a ton.
Finally with Alph Go and Alpha Zero you
have selfplay where the model plays
against itself learns from those games
gets better and better and better and
just like goes from something that's
like around human level performance to
like way beyond human capability. It's
like these go policies now are so strong
that it's just like incomprehensible.
Like what they're doing is
incomprehensible to humans. Same thing
with chess. And we don't have that right
now with language models. And so it's
like it's really tempting to look at
that and say like oh well we just need
these like AI models to now interact
with each other and learn from each
other and they're just going to like get
to super intelligence. The challenge and
I kind of mentioned this like a little
bit when I was talking about diplomacy.
The challenge is that go is this
two-player zero sum game. And two-player
zerosome games have this very nice
property where when you do selfplay, you
are converging to a minimax equilibrium.
And I I guess I should take a step back
and say like in two-player zero some
games, two player zero games are are
chess, go even two-player poker, all two
player zero sum. What you typically want
is what's called a minax equilibrium.
This is that that GTO policy. this
policy that you play where you're
guaranteeing that you're not going to
lose to any opponent in expectation. I
think in chess and go that's like pretty
clearly what you want. Interestingly, in
when you look at poker, it's not as
obvious. In a two-player zero some
version of poker, you could play the GTO
miniax policy and that guarantees that
you won't lose to any opponent on Earth.
But again, I mentioned there's you're
not going to to beat a weak player.
You're not going to make as much money
off of them as you could if you instead
played an exploitative policy. So,
there's this question of like, what do
you want? Do you want to make as much
money as possible, or do you want to
guarantee that you're not going to lose
to any human alive? What all the bots
have decided is like, well, what all the
like AI developers in these games have
decided is like, well, we're going to
choose the miniax policy. And
conveniently, that's exactly what
selfplay converges to. If you have these
AIs play against each other, learn from
their mistakes, they converge over time
to this miniax policy, guaranteed. But
once you go outside a two-player zero
some games, like in the case of
diplomacy, that's actually not a useful
policy anymore. You don't want to just
like have this very defensive policy and
you're going to end up with really weird
behavior if you start doing the same
kind of self-play in things like math.
So for example, what does it mean to do
self-play in math? you could fall into
this trap of like, well, I just want one
model to pose really difficult questions
and the other model to solve those
questions. You know, that's like a
two-player zero sum game. The problem is
that like, well, you could just like
pose really difficult questions that are
not interesting. You know, you just like
get ask it to do like 30digit
multiplication. It's a very difficult
problem for the AI models. Is that
really making progress in the dimension
that we want? Like not really. So
selfplay outside of these two players
here some games becomes like a much more
difficult nuanced question. So I think
and then Tim Tim kind of like basically
said something similar in his talk that
there's a lot of challenges in really
deciding what you're optimizing for when
you start to talk about selfplay outside
of two players here some games. My point
is that like this is where the AlphaGo
go analogy breaks down and not
necessarily breaks down but like it's
not going to be as easy as selfplay was
in AlphaGo. What is the objective
function then for that? What is the new
objective function? Yeah, it's a it's a
good it's a good question. Yeah. And I
think that that's something that um you
know a lot of people are thinking about.
Yeah. Um I'm sure you are. One of the
last podcasts that you did, you
mentioned that you were very impressed
by Sora. You don't you don't work
directly on Sora, but obviously it's
part of OpenAI. I think the the most
recent uh new updates or in that sort of
generative media space is auto
reggressive image gen. Is that
interesting or surprising in any way
that you want to comment about? I don't
work on image gen, so I my ability to
comment on this is kind of limited, but
I will say like I I love it. Like I
think it's super impressive. It's like
one of those things where, you know, you
work on these reasoning models and you
think like, wow, we're going to like be
able to do all sorts of crazy stuff like
advanced science and um, you know, solve
agentic tasks and and software
engineering. And then there's like this
whole other like dimension of progress
where you're like, oh, you're able to
like make images and videos now and it's
like so much fun. And that's getting a
lot more the attention to be honest,
especially in the general public. And
it's probably driving a lot more of the
like, you know, subscription plans for
CHBT, which is is great, but I think
it's just kind of funny that like yeah,
we're also I promise we're also working
on super intelligence,
but you can make everything gibbly. Uh I
think the the delta for me was um I was
actually harboring this thesis that
diffusion was over because of auto
reggressive imaging. Like there were
rumors about this end of last year and
obviously now it's come now it's come
out. Then Gemini comes out with text
diffusion and like diffusion is so bad
and like this is two directions and it's
very relevant for inference of auto
reggressive versus um diffusion. Do we
have both? Does one win? The beauty of
research is like you know you got to
pursue different different directions
and it's not it's not always going to be
clear like what is um you know the
promising path like and I think it's
great that people are looking into
different directions and trying
different things. I I think that there's
a lot of value in that exploration and I
think we all benefit from seeing what
works. Any potential in diffusion
reasoning let's say your channel I can
answer that. Okay. So you did a masters
in robotics too. Would love to get your
thoughts on one you know open kind of
started with the pen spinning trick and
like the robotic arm they wanted to
build. Is it right to work on the
humanoid likes? Do you think that's kind
of like the wrong embodiment of AI
outside of the usual, you know, how long
until we get robots, blah blah blah. Is
there something that you think is like
fundamentally not being explored right
now that people should really be doing
in robotics? I did a masters in robotics
years ago and my takeaway from that
experience, first of all, I didn't
actually work with robots that much. I
was like technically in a robotics
program. I played around with some Lego
robots my my first week of the program,
but then honestly I just like pretty
quickly shifted just working on AI for
poker and um was kind of nominally in
the robotics masters. But my takeaway
from like interacting with all these
roboticists and seeing their research
was that I did not want to work on
robots because the research cycle is so
much slower and so much more painful
when you're dealing with like physical
hardware. like software goes so much
more quickly and I think that's why
we're seeing so much progress with
language models and like all these like
virtual co-orker kind of tasks but
haven't seen as much progress in
robotics that like physical hardware
just is much more painful to iterate on
the question of humanoids I don't have
very strong opinions here because this
isn't what I'm working on but I think
there's a lot of value in nonhumanoid
robotics as well I think drones are a
perfect example where like there's
clearly a lot of value in That is that a
humanoid? No. But in many ways that's
great, you know, like you don't want a
humanoid for for that kind of
technology. I think weekly um I think
that nonhumanoids provide a lot of
value. I was reading u Richard Hammings
the art of doing science and
engineering. And he talks about how when
you have a new technological shift,
people try and take the old workloads
and like replicate them just in the new
technology versus you actually have to
change the way you do it. And you know
when I see this video of like you know
your humanoid in the house it's like
well the human shape is kind of has a
lot of limitations that could actually
be improved that I think people what's
familiar you know it's like would you
put a robot with like 10 arms and like
you know five legs in your house or
would that be yuri at night when you get
up and you see that thing walking around
and is that why we use humanoids. So I I
think to me there's almost like this
local maximum of like you know we got to
make it look like a human but I think
like what's like the the best shape uh
in house would be I'm terrible at
product design so I I am not the person
to ask on this. I think there is a
question of like is it better to make
humanoids because they're more familiar
to us or is it worse to make humanoids
because they're more similar to us but
not quite identical like I I don't know
which one I would actually find
creepier. Yeah. Yeah. The thing that got
me humanoid pilled a little bit was just
the argument that most of the world is
made for humans anyway. So if you want
to replace human labor, you have to make
a humanoid. I don't know if that's
convincing. Again, I don't have very
strong opinions in this field because
like I don't work in it. Um I was like
weekly in favor of humanoids. And I
think what really persuaded me to be
weekly in favor of like non-humanoids
was listening to um the physical
intelligence CEO and like some of his
pitches about like why they're not
pursuing why they're pursuing like
non-humanoid robotics. Okay. And
conveniently their office is actually
like very close to here. So if you
wanted to they're speaking at the the
conference I'm running. Okay. You know
I'd say like listen to his pitch and
maybe he can convince you that is the
way to go. Awesome. The other one I
would refer people to is Jim Fan
recently did a talk on the physical
tearing test which uh which he did at
the Sequoia conference which um was very
very good. Um he's such a great educator
and explainer of things. Um it's very
hard especially in that field. Um cool
we're done asking you about things that
you don't work on.
So these are just more rapid fires to to
sort of explore some of your boundaries
and get get some quick hits. How do you
or top industry labs keep on top of
research? like what are your tools and
practices? Uh it's it's really hard. I
think that a lot of people have this
perception that like academic research
is irrelevant and that's actually not
the case. I think that we do we look at
academic research I I think one of the
um challenges is like a lot of academic
research shows promise in their papers
but then actually doesn't work at scale
or even doesn't replicate. I think if we
find interesting papers like we're going
to try to reproduce that in-house and
see if it like still holds up and then
also does it scale well. But that is
like a big source of inspiration for us.
Whatever hits archive literally you do
the same as the rest of us or do you
have like a special process? Especially
if I get recommendations like we have an
internal channel where people will post
interesting papers and like I think
that's a good source of like okay well
this person that is more familiar with
this area thinks that this paper is
interesting so therefore I should read
it. Yeah. Um and similarly like I'll
keep track of things that are happening
in my space that I think are interesting
and like if I think it's really
interesting maybe I'll share it. For me
it's like WhatsApp and signal group
chats with researchers and that's it.
Yeah. I think it is like I mean a lot of
people look at things like Twitter and I
think it's really unfortunate that we've
reached this point where things need to
get a lot of attention on social media
for it to be paid attention to. Um
that's what the grad students are
trained. They're taking classes to do
this. I I do recommend to like you know
I've worked with grad students work with
fewer now because we don't publish as
much but when I was at fair publishing
papers like I would tell the grad
students I was working with that like
you need to post it on Twitter and you
need to and we go over like the Twitter
thread about like how to present the
work and everything and um there's a
real art to it and it does matter and
it's kind of the sad truth. I know when
you were doing the ACPC like the AI
poker competition, you mentioned that
people were not doing search because
they were limited to like two CPUs at
inference. Do you see similar things
today that are like keeping interesting
research from being done? That might be
it's not as popular. It doesn't get you
into the top conferences like uh are
there some environmental limiters?
Absolutely. And I I think one example is
for benchmarks that you look at things
like humanity's last exam like you have
these incredibly difficult problems but
then are still very easily gradable and
I think that actually limits the scope
of what you can evaluate these models
on. If you if you stick to that
paradigm, it's very convenient because
you know it's very easy to like then
score the models. But actually a lot of
the things that we want to you know
evaluate these models on are kind of
like more fuzzy tasks that are not
multiple choice questions and making
benchmarks for that for those kinds of
things is so much harder and probably
also like a lot more expensive to
evaluate. But I think that those are
really valuable things to work on and
that would fit the semment GBD 4.5 is
like a high taste model in a way.
there's kind of like all these like
nonmeasurable
things about a model that are really
good that maybe people are not well I
think there are things that are
measurable but they're just like much
more difficult to measure and I think
that a lot of benchmarks have kind of
stuck to this paradigm of posing really
difficult problems that are really easy
to measure so let's say the pre-training
scaling paradigm took about 5 years from
like discovery of GPT to scaling it up
to GPT4 and then we give you we give
test time compute 5 years as well So,
um, if test time comput a wall by 2030,
what would be the probable cause? It's
very similar to pre-training. We're
like, you can push pre-training a lot
further and it just becomes more
expensive with each iteration. I think
we're going to see something similar
with test time compute. We're like,
okay, we're going to get them thinking
instead of 3 minutes, they're for 3
hours and then 3 days and then 3 weeks.
Um, you run out of human life. Well, so
there's two there's two there's two
concerns. One is that it becomes much
more expensive to get the models to like
think for that long or like scale up
test time compute. Like as you scale up
test time compute, you're spending more
on test time compute which means that
like there's a limit to how much you
could spend. That's one potential
ceiling. Now obviously, well, not
obviously, but like I should say that
we're also becoming more efficient.
These models are becoming more efficient
in the way they're thinking is they're
able to do more with the same amount of
test time comput. And I think that's a
very underappreciated point that it's
not just that we're getting these models
to think for longer. In fact, if you
look at 03, it's thinking for longer
than 01 preview for some questions, but
it's not like a radical difference, but
it's way better. Why? Because it's just
like becoming better at thinking.
Anyway, yeah, these models um you're
going to scale up test on comput, you
can only scale it up so much. Like that
becomes a soft barrier in the same way
that pre-training it's becoming more and
more expensive to train better and
better pre-trained models or bigger
pre-trained models. The second point is
that like as you have these models think
for longer, you kind of get bottlenecked
by walk time. Like if you want to
iterate on experiments, it is really
easy to iterate on experiments when
these models would respond instantly.
It's actually much harder when they take
three hours to respond and what happens
when they have three weeks. It takes you
at least 3 weeks to do those evaluations
and to then iterate on that and and a
lot of this you can paralyze experiments
to some extent, but a lot of it you have
to run the experiment, complete it and
then see the results in order to decide
on the next set of experiments. I think
this is actually the strongest case for
for long timelines that the models
because they just have to like do so
much in serial time, we can only iterate
so quickly. How would you overcome that?
Well, it's it's a challenge and I think
it depends on the domain. So drug
discovery I think is one domain where
this could be a real bottleneck. I mean
if you want to see if something like
extends human life, it's going to take
you a long time to figure out if like
this new drug that you developed like
actually extends human life and doesn't
have like terrible side effects along
the way. Side note, do we not have
perfect models of human chemistry and
biology by now? Well, so this this is I
think the thing and again I want to be
cautious here because I'm not actually a
biologist or chemist. Like I don't I
know very little about about these
fields. I last time I took a biology
class was 10th grade in high school. I
don't think that there's a perfect uh
simulator of human biology right now.
And I think that that's something that
could potentially help address this
problem. That's like the number one
thing that we should all work on. Well,
that's one of the things that we're
hoping that these racing models will
help us with. Yeah. How would you
classify mid-training versus
post-training today? It's it's such the
all these definitions are so fuzzy. So I
I don't have I don't have a great answer
there. It's a question people have and
you're and like open eyes like now
explicitly hiring for mid-training and
everyone is like what the hell is
mid-training? I think mid-training is
between pre-training and post- training.
It's like it's like uh it's not it's not
post- training. It's not pre-training.
It's like adding more to the models but
like after pre-training like I don't
know interesting ways. Yeah. Okay. All
right. Well, you know I trying to get
some clarity.
Is the pre-trained model now basically
like a just an artifact that then spawns
other models and it's almost like the
core pre-training model is never really
exposed anymore and it's the
mid-training the new pre-training and
then there's the post-training once you
have the models branched out. you never
interact with an actual just like raw
pre-trained model. Like if you're going
to interact with the model, it's going
to go through mid-training and post-
training. So, um, so you're seeing the
final product. Well, you don't let us do
it, but you know, we used to. Well,
yeah. I mean, I guess if you, you know,
there's open source models where you can
just like interact with the raw
pre-train model. Um, but for for OpenAI
models, like they go through a
mid-training step, then they go through
a post- training step and then, and then
they're released. And they're a lot more
useful. Like, frankly, if you interacted
with a only pre-trained model, it would
be super difficult to work with and it
would Yeah. It would seem kind of dumb.
Yeah. But it' be it'd be useful in weird
ways, you know, because there's a mode
collapse when you when you post
straightforward for like chat. Yeah. In
some ways, you want that mode collapse
like you want you want that collapse of
like to be useful. I I get it. We're
interviewing Greg Brockman next. Uh
you've talked to him a lot. What would
you ask him? What would I ask Greg? I
mean I mean I get to ask Greg all the
time. What What should you ask Greg?
like to to evoke an interesting response
that like uh not he doesn't get asked
enough about but you know like this is
something that he's passionate about or
you just want his thoughts. I think in
general it's worth asking where this
goes, you know, like what does the world
actually look like in five years? What
does the world look like in 10 years?
What does that distribution of outcomes
look like? And what could the world or
individuals do to help steer things
towards like the good outcomes instead
of the negative outcomes? Okay, like an
alignment question. I think people get
very focused on what's going to happen
in like one or two years. And I think
it's also worth spending some time
thinking about like well what happens in
five or 10 years and what what does that
world look like? Um I mean he doesn't
have a crystal ball like but he he
certainly has he certainly has thoughts.
Yeah. So I think that's worth exploring.
Yeah. Okay. What are games that you
recommend to people? Uh especially
socially. Uh what are games that I
recommend to people? Uh I've been
playing a lot of this game called Blood
on the Clock Tower lately. Um what is
it? It's kind of like Mafia or Werewolf.
It's become very popular in San
Francisco is uh Oh, that's the one who
played in your house. Yeah. Okay. Got
it. Got it. It's kind of funny because
like I was talking to a couple people
now that have told me that it used to be
that poker was the like way that like
the VCs and tech founders and stuff
would socialize with each other. And
actually now it's shifting more towards
blood on the clock tower. Like that's
the that the thing that people use to
like um you know connect in the Bay
Area. And I was actually told that a a
startup held a recruiting event that was
a blood on the clock tower game. Wow.
Yeah. So, uh I guess it's like it's
really catching on, but it's a fun game
and I guess you lose less money playing
it than you do playing poker. So, it's
like better for people that are not very
good at these things. U I I think it's
kind of like a weird recruiting event,
but it's certainly a fun game. What
qualities make a winner here that is
interesting to hire for? That's the
thing is like okay I guess you get
ability to lie deception and like
picking up on deception like is that the
best employee I don't know.
So my slight final pet topic is Magic
the Gathering. So you have we talked
about some of these games Chesco and
they have perfect information. Then you
have Poker which is imperfect
information in a pretty limited
universe. You only have a 52 card deck.
And then you have these other games that
have imperfect information like a huge
pool of possible options. Do you have
any idea of like how much harder that
is? Like how does the difficulty of this
problem scale? I love that you asked
that because I have this like huge store
of knowledge on AI frame information
games like this is my my area of
research for so long and I know all
these things but I don't get to talk
about it very often. We've made
superhuman poker AIs for no limit Texas
holdem. One of the interesting things
about that is that like the amount of
hidden information is actually pretty
limited because you have two hidden
cards when you're playing Texas Holdem.
And so the number of possible states
that you could be in is 1,326
when you're playing heads up at least.
And you know that's multiplied by the
number of other players that there are
at the table, but it's still like not a
massive number. And so the way these AI
models work is they enumerate all the
different states that you could be in.
So, if you're playing like six-handed
poker, there's five other players. Five
times 1,326. That's the number of states
that you be. And then you assign a
probability to each one. And then you
feed those probabilities into your
neural net. And you get actions back for
each of those states. The problem is
that as you scale the number of hidden
possibilities, like the number of state
of of possible states you could be in,
that approach breaks down and there's
still this very interesting unanswered
question of what do you do when the
number of hidden states becomes
extremely large. Mhm. You know, so if
you go to Omaha Poker where you have
four hidden cards, there are things you
could do that's kind of like that are
kind of heristic that you could do to
reduce the number of states, but
actually it's still a very difficult
question. And then if you go to a game
like Strateggo where you have 40 pieces,
so there's like close to 40 factorial
different states you could be in, then
all these like existing approaches that
we used for poker kind of break down and
you need different approaches and
there's a lot of active research going
on about like how do how do you cope
with that? So for something like Magic
the Gathering, the techniques that we
used in poker would not out of the box
work. And it's still an interesting
research question of like what do you
do? Now I should say this becomes a
problem when you're doing the kinds of
search techniques that we used in poker.
If you're just doing model free RL, it's
not a problem. And my guess is that if
somebody put in the effort, they could
probably make a superhuman bot for Magic
the Gathering. Now, yeah, there's still
some unanswered research questions in
that space. Now, are they the most
important unanswered research questions?
Like, I'm inclined to say no. I think
there's like the problem is that like
the techniques that we used in poker to
do this kind of search stuff were pretty
limited. And like if you expand if you
expand those techniques, maybe you get
them to work on things like strategic
the gathering, but they're still going
to be limited. They're not going to get
you like superhuman and code forces with
language models. So, I think it's more
valuable to just focus on the very
general reasoning techniques. And one
day as we improve those, I think we'll
have a model that just out of the box
one day plays Magic the Gathering at a
superhuman level. And I think that's the
more important and more impressive
research direction. Cool. Amazing. Yeah.
Thanks very much for coming on, N. Yeah.
Thanks for your time. Yeah. Thanks.
Thanks for having me.
[Music]