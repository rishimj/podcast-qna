# Podcast Transcript
# Show: Signals and Threads
# Episode: What Is an Operating System? with Anil Madhavapeddy
# YouTube URL: https://www.youtube.com/watch?v=QQKD3ul0R0U
# Downloaded: 2025-07-19 20:58:06

[Music]
welcome to signals and threads in-depth
conversations about every layer of the
tech stack from chain street i'm ron
minsky
it is my pleasure to introduce anil
madhavapetti anil and i have worked
together for many years in lots of
different contexts we wrote a book
together neil and i and jason hickey
together wrote a book called real world
or camel we have spent
lots of years talking about and scheming
about o'camel and the future of the
language and collaborating together in
many different ways including working
together to found a lab at cambridge
university that focused on o'camel and
anil is also a systems researcher in his
own right an academic who's done a lot
of interesting work
and also an industrial programmer who's
built real systems with enormous scale
and reach we're going to talk about a
lot of different parts of the work that
anil has done over the years to start
with though i want to focus on one
particular project that you're pretty
well known for which is mirage can you
give us a capsule summary of what mirage
is shuriken and it's great to be here
ron
the story of mirage starts at the turn
of the century in the early 2000s pretty
much every bit of software that ran on
the internet was written in c and back
then
we had internet worms that were just
destroying and tearing through services
because there was lots of problems like
buffer overflows and memory errors and
reasons why the unreliability of all the
systems code that had been written in
the past was becoming really obvious and
the internet was really insecure so
there i was as a fresh graduate student
in cambridge and i decided that after
years of doing consistent programming in
c i would just have a go and see what it
was like to rewrite some common internet
protocols using a modern high-level
language and so i looked around and i
looked at java which was obviously the
big language back then i looked at perl
which was heavily used for scripting
purposes but in the end i decided i want
something that was the most unix-like
language i could find and i ended up
using ocamel it had fast native code
compilation that just ran in unix it
could be debugged very easily it had a
very thin layer to the operating system
i spent a great couple of years figuring
out how to write really safe
applications in ocam so i started by
rewriting the domain name service which
is how we resolve names human readable
names like google.com to ip addresses
and i rewrote the secure shell protocol
which is how most computers just talk to
each other over remote connections and i
rewrote all of these in pure camel and i
showed this part of my phd research that
you could make these not only as high
performance as the c versions which
really wasn't that well known then
because there was a perception that
these high level languages would be
quite slow but then i also showed that
you could start doing some high-level
reasoning about them as well you could
use model checking and early
verification techniques to prove
high-level properties and this was all
really good fun i wrote loads of a camel
code and then i published all these
papers and then i asked myself a simple
question so i've written all of this
code to rewrite network protocols and
and have safe applications
but then the compiler just seemed to
stop so after all of these beautiful
abstractions and compilation processes i
got a binary at the end and this binary
just talked to this operating system and
i might have written a hundred thousand
lines of a camel but this operating
system had 25 million lines of c code
the linux kernel
so why after all of my hard work and
perfecting this beautiful network
protocol do i have to drag along 25
million lines of code what value is that
adding to me when i've done so much in
my hell of a language and this is where
mirage os comes in so mirageois is a
system written in pure camel where not
only do common network protocols and
file systems and high-level things like
web servers and web stacks can all be
expressed in a camel
but the compiler just refuses to stop we
then provide different abstractions to
plug in the actual operating system as
well and so the compiler instead of
stopping and generating a binary that
you then run inside linux or windows
will continue to specialize the
application that it is compiling and it
will emit a full operating system that
can just boot by itself the compiler has
specialized your high-level application
into an operating system that can only
do one thing the thing that is written
to do and it does this not just by
looking at the source code it also looks
at your configuration files which are
also written in a camel it evaluates all
of those in combination with your
business logic and then it compiles a
whole thing in combination with
operating system components written in a
camel like tcpip stacks and low-level
file systems and network drivers and
those kind of things and it's what's
known as a unikernel the unit kernel is
a highly specialized binary output so
mirage os started off as an experiment
in my phd 15 years ago i've been joined
by an incredible community initially by
thomas resigned and david scott and now
by a large mirage us core team and we
have hundreds of protocols and file
systems and pieces that can all fit
together and be combined into very
bespoke
artisanal infrastructure so you can
design a kernel that does exactly what
you want it to do and you don't have to
drag along other people's code unless
you want to maybe an overly pithy
summary of this is your operating system
as a library well so this is an
operating system but what is an
operating system in a normal operating
system you run a bunch of processes it's
known as userland where this is a
failure domain where something goes
wrong or it needs resources from the
outside world it's got to ask a higher
level system and the higher level system
in conventional operating systems is the
kernel so the linux kernel for example
is the thing that mediates all of the
resources in your system it manages the
hardware and it acts as a middleware to
give software
safe and isolated and high performance
access to the underlying hardware
so with unikernels it uses a different
approach of structuring operating
systems one known as library operating
systems and this is one where instead of
the kernel acting as a big wrapper
around all of your code
it simply is provided as a set of
libraries so there's no different from
any other library that you link to such
as openssl or some kind of graphics
library for example the kernel is just
another one of those things but what you
sacrifice is multi-user modes because if
one application is accessing some system
libraries it needs exclusive access to
the hardware it's quite hard to provide
competing or untrusted access to
different parts of your hardware stack
so library operating systems work really
well if you're trying to build a
specialized application that is
maximally using the hardware at hand if
you just want to have a desktop with
lots of applications running then you
should just use conventional operating
systems it's only if you can benefit
from the specialization that you want to
switch into this different mode of
operating system construction
what i like about mirage os as an idea
is it's so weird like it's hard to know
whether it's a research project or a
stunt and it's also i think part of what
i think of as a larger story of the
multi-decade-long failure of the
original idea of an operating system
back in the day we had this idea of what
we're really going to do is build
multi-user operating systems computers
were really expensive and we need to
share them share one big computer among
a bunch of people until we built systems
like multics and then systems that took
inspiration from them like unix and lots
of other systems along the way and we
built all of these abstractions that
were designed to make it easy to share
hardware among multiple people and to do
it safely and then in the last couple of
decades we have
more
completely and utterly given up on that
project and
things like virtualization and
containers are all examples where we're
like no no that's not what an operating
system is for operating systems are for
piling up your complicated stack of all
the different components you need to do
when you throw together to build an
application and you want to kind of add
them up and freeze them in place
so you can replicably build up this
weird agglomeration of stuff that you've
thrown together the original purpose of
actually having multiple users share the
same operating system has basically
vanished from the scene and once you've
made all those changes the idea that
instead of all of the traditional
abstractions that we needed when we were
separating out different users maybe we
could do something radically different
that's where i see something like mirage
os showing up that's right it's an
interesting perspective to think that
operating systems have been a failure
because what's really happened in the
last 20 or 30 years is that we have
invisibly added layers that provide the
right level of abstractions needed for
that point in time so for example in the
late 90s i would spend ages building a
beautifully configured windows machine
because i knew exactly all the registry
keys and all the magic that went into it
but in the early 2000s i worked on the
zen hypervisor and the zen hypervisor
started off with a very simple thesis
which is it is possible to run multiple
instances of an operating system not
designed to run on the same physical
hardware simultaneously and make sure
it's completely isolated from the other
operating systems running on the machine
but also do so with minimal performance
it was a serious balancing act there and
so what we did with the zen hypervisor
was
don't touch anything in user space
because you don't want to have people
rewriting all their applications their
oracle databases or their sql servers or
whatever they're running so we scooped
out the guts of the kernel and normally
the guts of the kernel in linux is what
manages the low-level hardware the
memory management subsystem the
interrupt controller and the things that
map hardware to operating systems
and with this simple modification we
adopted a technical power virtualization
and what power virtualization did was it
just fooled the kernel into thinking it
was running a real hardware but we
shimmed in a little layer called the
hypervisor the zen hypervisor which then
did all the real mapping to real
hardware it turned out this was
extraordinarily effective because we
could take entire physical operating
system stacks and tens of millions of
lines of code all combined and run them
simultaneously in a single physical
machine and make sure that they were all
utilized to their maximum potential so
if you had a bunch of machines all being
used 10 of the time we could shove these
in one place now this was worked out so
well because the notion of a user wasn't
someone who's logging into a windows
machine but it became the person who's
booting up an operating system and then
suddenly the zen hypervisor became its
own operating system and cloud computing
and all of these kind of things took off
by the mid-2000s but they just provided
a different interface and when mirage
came along it was kind of the leftover
portions of of the zen experiment
zen also interestingly started off as a
stunt it was a bet in the castle pub in
cambridge that carefraser couldn't hack
linux over a weekend and then monday
came along and we had the first version
of zen and then a big team of us
continued working on it i then spent the
next few years at a startup company
called zensource
building all of the support to make it
production quality so we could sell the
zen hypervisor as a product so that we
had windows drivers and linux drivers
and
those years were filled full of
compatibility woes so you have to look
at every single edge case and make sure
it works perfectly and then
life just got frustrating you just get
bored of making other people's code work
well in your virtualization layer
so we had to have some way to test zen
and so mirage os the first version of it
came along because we built a minimal
operating system that didn't have
all of the windows baggage and all the
linux baggage and all it did was
exercise the lowest levels of the zen
functionality the device drivers the
memory subsystem and so on i needed to
have slightly more complicated tests so
with toma gazinger we just linked into a
camel runtime because we just needed to
write some high-level logic and then
that was running inside the zen
hypervisor as a minimal operating system
so it was a few 100 kilobytes in size at
most
and then we're sending ethernet packets
so wouldn't it be nice if you could just
hook up another camera library to send
tcp frames instead of low-level ethernet
so then i started writing a tpip stack
in in pure camel and then you know once
you have tcp it's a pretty small step to
go write an http stack in a camel and
then that happened so mirage us became
this kind of organic growth of starting
from low level interfaces figuring out
what the system abstractions that we
need are and then filling in the blanks
with libraries so it did start as a
stunt i think all good systems projects
start with a stunt because you're trying
to test an experimental hypothesis
you're trying to show that if we modify
the world to be the way we wanted to be
with our hypothesis this is worth doing
and you need that stunt to show that all
of the effort and all the hard work that
goes into productizing something is
actually worthwhile so zen the
hypervisor was a stunt just to show that
you could just boot three linuxes on one
machine and then it it to this day it
remains one of the industry's most
popular hypervisors and mirage os also
started as a stunt just to show you
could build a credible sequence of our
camel applications and protocols and
compose them together
and build something useful mirage os
today has hundreds of tens of millions
of daily active users it's embedded in
all kinds of systems that use
the libraries and the protocols in lots
of different ways
and it's invisibly servicing lots and
lots of cloud infrastructure yeah i
think it's hard to overstate how
impactful the zen work has been it's the
foundation on which the entire modern
internet is built right the
virtualization is absolutely at the core
of what an enormous number of companies
have done an enormous number of
different systems that have been built
have been built on top of this there's
been a bunch of ways that mirage os has
gotten into big and important
pieces of infrastructure one thing i
wonder about is are you happy with the
set of abstractions that we've started
to build up around this in some ways i
feel like the stunt-like nature of all
of this shows a little bit in the
happenstance of what we got a lot of the
things that we've ended up building are
things that you could kind of shim in
right we started off building a big
multi-user set of operating systems and
we're like oh actually the abstractions
aren't good enough for supporting
multiple users truly isolated from each
other so we started doing
this in some sense very strange thing
where we said we know what's the right
abstraction
hardware like whatever the physical
hardware happened to provide at the
bottom layer that's the thing that will
allow us to take our operating systems
and just port them
cheaply to new places so let's pick
hardware as the new abstraction and i
find it hard to believe on some level
that either of these are really good
choices if you were to actually start
from scratch in a way that's not just
like a stunt but like a multi-decade
long commitment to rebuild the entire
world do you have a feel for what
abstractions you'd actually pick that's
a great question so mirage is now
15 years old
and we are never happy with our
abstractions i don't think there's been
a single day where the core team has sat
down and said we have the perfect set of
interfaces that will survive for the
next few years and it's worth stepping
back a little bit to explain why o'camel
was the right choice from raj os and and
why it empowers this continuous
evolution of our interfaces in our camel
you have the notion of
modules and this is one of the defining
features of a camel beyond being a
functional programming language and what
modules do is that they let you define
an interface and this interface is a
series of types which can then have
functions that operate over those types
and and that collection is known as a
module signature and
whenever in mirage os we are defining
some abstract hardware or even a
high-level thing we define a module
signature for this thing and all that
does is sketch out what goes in and what
goes out and how you create things of
this module type
but then in ocamel you also have this
notion of in module implementations
modules themselves and if they satisfy
that module signature then you could
apply this in a type-safe way and you
can compose lots of lots of different
module types with lots of lots of
different implementations
in mirage we have a sequence of module
types which represent the full set of
our possible hardware and application
level and protocol level signatures but
then we also have hundreds and hundreds
of concrete libraries which satisfy some
of those module signatures so for
example if i have a networking module
signature that just says you can open a
connection and you could read and write
from it we call this a flow in mirage os
then there are several possible
implementations of this flow interface
one of them is just a normal linux
socket stack which will compile only on
linux and another one is a full or camel
based implementation of tcpip
which exports the same socket interface
but instead of delegating the
requirement to actually send the network
traffic to the kernel it actually
implements it in pure camel and so in
mirage os whenever we're not happy with
the lack of some safe code we go right
in implementation whenever we're unhappy
with the evolution of some hardware
interfaces or virtualization interfaces
we go rewrite our module signatures and
all we have to do is to adjust our
implementations so that they match the
new module signatures we can do this in
an incremental and evolutionary way and
so over the years we've learned a ton of
stuff we've seen an evolution of
hardware both in terms of performance
and straight line capabilities we've
seen it change in terms of the security
model we started with just page tables
for memory and now we have all kinds of
trusted encrypted
memory enclaves and we have nested
virtualization it's become an incredibly
sophisticated interface there and then
we also have the dimensionality of
distributed systems which is just
another way of programming and
abstracting across the failure domain so
ocamel lets us split up our
implementations and our signatures into
two discrete halves and then try to
evolve uh continuously and that's why
the mirage project is called mirage
because our idea was that the mirage
project would disappear and just become
the default way that people programmed
systems because our signatures would
just become part of the standard
community and part of the standard way
that people build things and we've been
seeing that over the last few years
and one i think subtle advantage of
mirage which is not i think totally
obvious to someone who encounters it as
an operating systems project is you can
take a program that was built for mirage
and you can run it with an ordinary
operating system your point about one of
the ways that you can get network
services is to just use the standard
network services on the operating system
of your choice
and the other way is to have a pure
camel implementation that goes all the
way down and run that inside of
hypervisor or maybe run it inside of on
an actual bare metal server
right so there's an enormous amount of
flexibility in terms of how you take
these things and deploy them this may be
not obvious if you just think about it
as an operating system in some sense
it's both more than that and kind of
less in the sense that you know as you
said there's a way in which the more you
look at it the more you wonder like what
actually is here in some sense the whole
architecture disappears into the
background that's right
that's right well to give you a concrete
example of this right now we're really
worried about climate change so we
thought we would build a website that is
purely solar-powered and one observation
about websites for example their camel
labs website is that most people
probably only look at the website when
it's daytime right there's not much
machine access to the website so we
thought well what if we had a bunch of
raspberry pi's around the world that
were just solar-powered and so the
process of writing this kind of thing is
first of all just start writing it in
unix like a normal or camel unix
application and we built the web server
with my colleague patrick ferris
and then at this point we start
measuring the energy usage and the
energy usage is high because it's
running linux in the raspberry pi and
then it's just taking up more budget
than our solar is letting us provide
so
then we wrap it in a more constrained
mirage os interface so one that doesn't
give you the full access to linux and
all the syscalls and only requires a
small file system and so this is just an
evolution over our existing linux code
and then suddenly it becomes compatible
with all of the direct unicorn
interfaces and then you can replace the
raspberry pi with an esp32 one of those
tiny little 32-bit microcontrollers and
your energy budget drops dramatically
but obviously your capabilities drop but
i had the luxury of developing the
raspberry pi environment which is a full
linux environment and then when i decide
well okay my high level logic is right i
can bisect it and then get rid of the
lower half of the operating system it's
all just done through iterative normal
pure camel development it's worth noting
as well that anyone can build their own
custom kernel if you've never done any
kernel hacking
you can still use mirage os programming
pure camel and have a custom kernel that
you can boot it is really quite dramatic
if you think that there's a mystique in
kernel programming because there isn't
it's just another very very large
program that is hard to debug so i think
i have a pretty good sense of what's to
like about this approach
one advantage is that you get all the
flexibility that you get out of a
powerful programming language for
building rich abstractions in a kind of
kernel environment you are restricted in
various ways to building abstractions
that are in some sense safe via the
hardware support that you have for
separating kernel code and non-kernel
code there's a bunch of constraints
about how you can build that kind of
system here you get to use the
abstractions very freely you can build
just what you want and you can have a
compilation process that just doesn't
link in the stuff that you're not using
so you get things that are truly minimal
and as a result more secure so that all
seems really exciting yeah i have an
enormous amount of sympathy for the idea
that part of the way that you make your
world better is by extending the
programming language i think this is a
luxury that jane street has had over the
years and i think that in some sense
everyone whether they know it or not is
enormously dependent on the
fundamental tools they use including the
programming language and people mostly
think of themselves as being in the
position of victim with respect to their
programming language of choice they
mostly use it and don't have a lot of
control over how it works but being in a
place where you can be in real
conversation
with the community of developers that
defines the language lets you when you
find really important ways of changing
that ecosystem actually being able to
push that forward that's a very powerful
thing it is and or camel in my mind is a
generational language one of the
properties i want from systems i build
is that they last the test of time
so it's so frustrating that a system i
built in the early 2000s if you put on
the internet today would be hacked in
seconds it would just would not survive
for any length of time
so how do we even begin the discipline
of building systems that can last for
forget a decade just just even a year
without having some kind of security
holes or some kind of terrible terrible
flow now there is one argument saying
that you should build living systems
that are perpetually refreshed but also
we should have the hope of building
eternal systems that have beautiful
mathematical properties and still
perform useful utilitarian functions in
the world
so there's one big downside i feel like
i see in all of this which you haven't
talked about yet which is it requires
you to write all of your code in ocam
and you know i really like o'camel you
really like go camel it's in some sense
not a downside but if you're trying to
build
software that's broadly useful and
usable and can build a big ecosystem
around it
restricting down to one particular
programming language can be awkward i
mean just to say the obvious
i would find it somewhat awkward if
there's some operating system i wanted
to use and i had to use like whatever
their favorite language was and i
couldn't write in my favorite language
how do you think about this trade-off
totally well first of all we must use
multiple languages it's not really
o'camel that is the lure for this notion
of generational computing it's the fact
that there's at the heart of it a simple
semantic that could be expressed in a
machine specifiable form
and although you know we have the
chemical syntax and everything at the
heart of it there's no formal
specification about camel but it's
obvious that one is emerging and one can
be written in the next certainly five
five to ten years and this means that
once you have a large body of code that
has semantics it has meaning it's
possible to transform it into other
languages and other future semantics and
that kind of self-description is a
really really important part of the
reason why i chose a camel it's still
possible to compile code i wrote in the
early 2000s using the modern or camel
compiler so i've compiled code i wrote
20 years ago in fact it was camel's 25th
birthday just a few months ago and i
tested out the first program i could
find it was my cvs repository and it
compiles fine
but when you want to use another
language then we just go through the
foreign function interface and it's just
like that process abstraction i talked
about all you have to do is spin up
another process which is another runtime
and you have to talk to it and the
industry has made tremendous progress in
understanding how multi-language
interoperability should work
specifically through webassembly for
example at the moment we have a
substrate where modern browsers can run
quite portable code but more importantly
than the bytecode is their emerging
understanding of what it means to make
function calls across languages and all
we have to do is take advantage of
whatever those advances are and we can
link multiple libraries for multiple
languages together so again it's a
mirage right by using other people's
advances mirage can benefit because all
we need are libraries
to build these operating systems nothing
else
everyone loves libraries everyone has
them that's the only thing we need and
standards for how they can talk to each
other one of the things that i think is
really important about programming
language design is building a good
programming language it is as much about
what you leave out as what about what
you put in
and having a set of abstractions that
smoothly work together language features
that really click where it's really easy
to use other people's code no matter
which subset of the language features
they try to use and they'll still all
hook together it's hard to build a
language that encourages that kind of
simplicity that embodies that kind of
simplicity and
if what you need is now languages that
need to kind of be fully interoperable
with each other
there's a degree to which each language
has to fully embrace the complexity of
the other languages
and it can get awkward fast i wonder if
some of the
simplicity that mirage offers
would get harder to maintain in a
context where you're trying to have lots
and lots of different languages
interacting with each other it
definitely does because you're trying to
get end-to-end guarantees so one of the
big users of mirage unicorns is the
tezos proof of state blockchain and
tesla's is a complicated distributed
system with lots of nodes and validators
and security keys flying around so to
build that as a unit kernel involves a
lot of a camel code it's a larger
channel code base but also rust code
there's been really interesting work on
hooking together the rust type system
which is based around a borrowing model
so there's a lifetime model for how long
values persist and their camo model
which is based around garbage collection
it involves dynamic collection but this
works because
typically the rust code is at the lowest
levels of the system it's kind of at the
runtime part of the system so as long as
you have a clean layering where you're
starting from a c runtime then you're
moving into the rust code which is very
unopinionated from a garbage collection
perspective but very opinionated from a
lifetime perspective and then calling
into the camel code things work out
pretty well we made tremendous progress
in building some really complicated
unicorns from a very very complicated
distributed system but you have to just
make sure you look at your entire
language stack and your dependency stack
ahead of time make sure you understand
how they interoperate at a high level
and then dive into turning into the unit
kernel so it's definitely not a magic
wand that you can just wave and expect
the build systems to just work another
example that we use mirage for is in
docker which is a container management
system and if you've ever used docker
for mac or dock for windows then every
byte of every container that you're
using in your desktop is going through a
mirage os translation layer
because whenever you mount a file system
on the mac for example something has to
translate the semantics of your mac file
system which is apfs or hfs
into a linux container which is a
similar looking file system but actually
completely different under the hood
and so what we did was we did a very
special mirage dave scott's david sheets
and jeremy yellow they figured out that
if you treat one end of a mirage
compilation target as linux and the
other end as mac os we can build
translation proxies simply by
serializing network packets into a camel
into their camel stack and then
deserializing it on the other end and
turning it into socket calls so now the
mac transparently reconstructs traffic
coming out of a container and then emits
them on your mac desktop as normal mac
networking calls so a lot of the tricky
difficulties of network bridging and
firewalls and all of that stuff just go
away so when you run a linux container
on the mac it goes through mirage os and
it looks just like a mac application
when we deployed that in docker i think
our support calls went down by about 99
so anytime this software was deployed in
the enterprise everyone's got some crazy
firewall and antivirus software and
things that break some integration of a
virtualization stack with your system
today a dock for mac you just double
click on it you install it on record
windows and it's like a background
daemon that just runs in the system with
minimal interruption so and that's the
user experience we were going for it but
it's only possible because again we
understood how to interface go with a
camel but made sure we did it in exactly
the right order then once you deploy it
it's incredibly robust in production but
you just have to take the time to make
sure you understand the lifetime of go
values the lifetime of camel values and
make sure they can interoperate
correctly and this is another example of
the flexibility of mirage right it's not
just an all at once operating system it
needs to know everything and then you
run it on bare metal like here you are
integrating it as a very carefully
designed shim between two operating
systems running on the same machine
that's right so along the way kcci ram
christian joined our camera labs to work
on multi-core parallelism hannah's
menhert from rober and david caliper
were on a beach in morocco and they
wrote us a tls stack
and then they did this incredible stunt
where they decided they loved mirage and
they'd never talked to me or any of the
mirage team and on this beach in
marrakech they wrote a complete ssl
stack in the wake of the heartbleed
attack and then they put up what we
called a bitcoin pinata and this bitcoin
pinata was in about 2015 or so i think
they hid 10 bitcoins inside a unicorn
put it on the internet and they left the
private keys inside the unicorn and they
said to the internet if anyone could
break into this unicorn and take those
keys and trade those bitcoin we can't
deny the fact that this thing has been
hacked and you can keep the money so
back then i think you know bitcoin was
worth not very much but then during the
course of the experiment there was
hundreds of thousands of attacks against
this system and it got on hacker news
and all of you all the social media
networks people kept crashing the system
by dealing with servicing it but then
like a real pinata it just bounced back
and rebooted in 20 milliseconds because
that's how long a unicorn takes to
reboot and it was back up again and no
one managed to take the bitcoin in the
end i think we donated to charity
because it was growing a bit much but it
just goes to show how
you can assemble all these things
you can get a community who can then do
what they want to do with it and then
contribute back to the whole so today if
you use a tls stack you know camel or
indeed an http stack you're probably
using one of the mirage libraries
there's many many alternatives but for a
long time the mirage libraries became
the de facto community stacks that
people used right and i would assume
that mirage in its various forms maybe
mirage plus zen
together
are responsible for most of the
deployments of o'camel code onto
people's actual machines how many
machines do you think software that
you've worked on has now been installed
on it's a hard question to answer
because we're deploying products so
there was an ocamel zen story which is
the management demon behind zen store
which i believe amazon used for many
years so that would cover quite a lot of
machines in the cloud i can't say
exactly how many but a lot and then
docker for mac and docker windows i
think was the second most popular
developer tool behind visual studio code
so it's deployed on tens of millions of
desktops for sure but then of course in
the community you have people like
facebook who have written their front
end for their messenger application in a
variant of a camel known as reason ml
and compile that to javascript so that's
also to some extent deployed but not
deployed in the same way that's a good
point that might be more desktops than
all of the docker desktops in fact it
kind of has to be it does that would
probably address a few billion desktops
but it's a website right it's not an
application running on the other side
but our plans right now are even bigger
i'm working on some climate change
projects where we need to deploy
millions of sensors around the world and
of course we're using mirage to deal
with the complicated logic of carbon co2
sensing and chemical tasting and
deploying it in risk five hardware
that's quite embedded so the mirage
journey is just continuing but on
different paths and different use cases
we have in germany the robo team
deploying all kinds of different unit
kernels for the german government i
think they have a contract to build
secure vpn tunnels and lightweight
overlay networks and all of these are
unit kernels that are being deployed so
who knows how far it's going to go
inside critical infrastructure on the
internet in the coming years so a thing
i've always found striking about your
background is you've dug deeply into a
bunch of different areas you've done a
lot of different open source work over
the years of various different forms
you've done lots of impactful academic
research and you've been involved in a
bunch of pretty major industrial
projects can you tell us a bit about how
you got
into this whole line of work in the
first place how did you get into
computers and into systems research
where did this journey start
well i'm actually not a computer
scientist i began my training as an
engineer and i actually plan to get into
electrical engineering i was fascinated
by power systems and cars and planes and
so on
but then when i was studying in london i
got working on a computer game an online
mud where you could program this game
and it was programmed in a really
interesting language called lpc which is
kind of a pseudo-functional
object-oriented language from the late
90s and i went to a party it was known
as a mud meat and i got drunk and
and i woke up the next day and i'd been
offered an internship at nasa to work on
the mars pool of lander and this was in
california it was an exotic land far
away from gray and dreary london so i
ended up that summer working on the
various bits of infrastructure for
helping the mars portal under land and
when it finally landed this was the
first time that we had the technology to
live stream the photographs that were
coming out of mars i was kind of set up
i would say as the person who set up all
the infrastructure for supporting one in
three people on the internet to access a
website all at once because the world's
attention was focused on this landing in
1999
so i rapidly learned how computers
worked and stuff and operating systems
and things and i set up all of these
solaris boxes and the first thing that
happened was those boxes got hacked so i
put them up on the internet and
obviously hackers love mars.nasa.gov as
a domain to control and so they took
them over and i then looked around for
more secure alternatives and i found
this operating system called openbsd and
what openbsd is it's an all-in-one
operating system designed with
reliability and correctness in mind and
use a variety of security techniques i
wiped all of these expensive solarize
boxes installed openvsd and then managed
to get the system running stably again
and then openbsu is open source so i
found a few bugs because when you're
deploying something as large as that you
can't not find some bugs right and it
turns out that i could just send in some
patches and they got interested and they
accepted my patches
and this is like some massive dopamine
rush because when someone takes your
code and incorporates it into this
operating system used by loads of other
people it's an incredible feeling i got
it more and more into that development
and i ended up going to an open bsd
hackathon and these are regular
semi-annual events and back then it was
in calgary in canada because the us
export restrictions prevented any
cryptographic code from being written in
the u.s so i got to travel and go to
canada and then talking to damian miller
who's one of the core maintainers of ssh
it set me on the path to thinking well
how can you start rewriting systems in a
more secure fashion and then i went back
to cambridge because the mars pollinator
crashed straight at the bars at very
high speed so all of the infrastructure
we said i've never actually used well it
got into cnn and lots of people looked
at our sad faces people got to watch the
crash due to your hard work people got
to watch the crash we had to wait for
like two days until you know until we
decided to crash so people stopped
watching after about five minutes but we
waited two days and then i had to find a
new job because i was so depressed that
all of our hard work had hit mars at
high speed and so i decided to go back
to cambridge and do a phd and then i
really started my training as a computer
scientist so during the phd i did lots
and lots of different projects but i
started working the zen hypervisor i
started using a camel in functional
programming more seriously in order to
build the stacks that i described
earlier and then it became this
wonderful journey where all of the code
i've ever written has pretty much been
open source a lot of it's terrible but
it's been included lots and lots of
products it's really easy to move
between industry and academia and
government jobs because you're kind of
taking your secret weapons with you
wherever you go so now it's not like i'm
obsessed with a camel it's just the most
efficient thing for me to use to solve
any given problem because i've just
deployed in so many contexts that if i'm
doing anything for building my website
or doing a bit of data processing it's
just what i reach for it's a really fun
thing to work with even after all these
years and you've talked some about why
you think oh camel is a good fit for
mirage and what you're trying to do
there but ocamel's not a tool that
systems programmers reach for early
how did you end up coming across it in
the first place well in cambridge or
camel is now taught to first your
students because first of all it's kind
of a reset button because
most students would come with a
background of javascript or python and
they'd have partial knowledge so we
wanted to find something that's a little
bit obscure but certainly not massively
in the mainstream secondly it's the
easiest way to teach the foundations of
computer science so the basics of data
structures and recursion and
representations and all the beautiful
logics and proofs that follow from that
so at cambridge there's a long tradition
of using ml style languages from
standard ml to a camel so i couldn't
help but be exposed to it because of the
university environment secondly it was
also the most practical way to do
systems programming in the early 2000s
so there weren't really any other
alternatives back then you could go for
java which is very heavyweight you go
for pearl which was right once it still
is to some extent python and ruby were
still very much in there at fledging
phases there weren't many other compiled
languages so today we had this wonderful
spring of programming languages but we
didn't back then
but languages have momentum as well so
this is a generational concept to keep
going back to it's not like we're just
avoiding other languages but when you
build up such a large code base of a
camel code it just gets easier and
easier to build and advance it every
single day so it's almost at the tipping
point now where it's easier to extend or
camel with rust style features than it
is to rewrite all of our code in rust
for example or in any other language
that comes along it's easier to go do a
machine proof in using the proof
assistant and extract to a camel than it
is to do anything else and so
it's this reduction of friction that
just builds up over the years i
understand what you're saying but i feel
like what you're saying is also on some
level objectively false
meaning you're saying like well you know
back in the 90s what systems programming
languages were there other than o'camel
and i'm like there was c and in fact
that's what everybody used right it is
not the case that system programmers in
general in the 90s looked around and
were like oh yeah we're definitely going
to write all our systems no camel no
that's right if i could go back in time
i would evangelize a couple not now but
in the late 90s because i feel like i
missed a lip of innovation there no one
had heard of a camel back then and it
was just this incredibly productive tool
to write unix-like code it was just
better than writing in c and this is me
emerging out of writing lots of secret
for many many years and indeed writing
lots of php code for websites and
webmail stacks and so on but ocamel went
through a period of stagnation because
like any open source project if it's not
invested in if it doesn't have a large
body of programmers then it's really
hard to sustain it over the years so
around 10 years into camel's life
which is roughly when i was using it in
about 2005 the rate of progress really
stalled and so at this point we kind of
missed a window where we could have
heavily evangelized this to more systems
programmers didn't have the tools and
the right development environment to
make it easily possible so while we used
it heavily at zensource it never got
picked up by other developers within
zensource because of that lack of
tooling so we talked some about your
background in open source
some of the work that you've done in
fact that you and i have collaborated on
over the years has been about developing
the open source community around o'camel
and helping in part certainly not just
us but helping in part to kind of combat
some of that stagnation
and part of that was the creation of
ocam labs
can you tell us a little more about
where ocam labs came from i can so
whenever we finished at zen source it
got acquired by citrix and i left her
for a few years of happily hacking on
zen within citrix i went back to
academia and i knew that i had this
burning desire to build mirage os
because everything was set i had all the
code from the previous startups i had
the problem i had five years of funding
i had this wonderful research fellowship
to work on but it was just me and i knew
that if i wanted to make this as big as
i wanted it to be i needed help and it
was helping multiple fronts the first
thing was that the ocam development team
was incredible i remember having dinner
with zebeluwa in about 2009 and he just
said that they would maintain a camel
forever but they were struggling with
all of the bug reports coming in and the
fact that they didn't have any dedicated
staff working on it but he said you know
anyone can work in it but why isn't
anyone doing it i got talking to europe
and we said well why don't we find
someone that will help us do this and it
was really hard to find anyone who would
actually work in the core compiler
look at bug reports and build our
tooling because these are all the things
that we needed in the end it came to a
hard decision if you can't find anyone
else then perhaps i should do it myself
and the reason i was really motivated to
do this myself was because i wanted this
for mirage os so anything i did to
improve o'camel would directly leverage
and improve mirage os the project i'm
really passionate about so we funded a
camel labs in cambridge and one of the
beautiful things about cambridge
university is that individual staff
retain their intellectual property it's
not owned by the university and so this
meant that working in open source became
really easy because anyone we hired at
the university could just write code and
there wasn't any need for any legal
agreements or anything with the
university we just released it so i'm
really really proud that what we started
with a seed in cambridge has now become
a diaspora of people all around the
world working in different geographies
in different environments but continue
to communicate and share their code
through the open source ecosystem and i
think cambridge as an institution
deserves an enormous amount of credit
for all of this because
this thing was messy and complicated and
does not fit in in an ordinary way
to a kind of simple notion of academic
research a lot of the work that needed
to be done was work about coordinating
open source ecosystems and
maintainership work it's not the kind of
stuff that gets you tenure
most institutions aren't willing to take
it on and cambridge was and i think it
was important to have an academic
institution that's willing to do it
because o'camel is
in many ways a deeply academic language
its roots and much of the expertise
just realistically resides in academic
institutions
there's an enormous amount of connection
to various different kinds of
real and legitimate research work we saw
lots of exciting things coming out of
cambridge on a kind of research side
that were secondary to this and all of
this other real infrastructure that was
created we looked around and tried to
find various homes for o'cama labs and
cambridge was the place it was willing
to do it it was an enormously important
find that we found an institution that
was really willing to partner with us
effectively in doing this kind of work
another thing that strikes me about
the story you're telling is
the degree to which ocamel labs
acted as a kind of effective form of
glue like a lot of the work you're
talking about which is important
advances in the state of the art for a
camel they're not all things that were
done at ocam labs right merlin was
created by some inria undergrads if i
remember correctly but they were later
working with and supported by ocam labs
oh camel format was just done as an
internal facebook project and then jane
street adopted it and made a bunch of
further changes but it was a camel labs
that provided the glue to kind of take
it and turn it into a maintained and
general purpose piece of software and
figure out how to kind of share between
the various different contributors dunes
another example dune was created at jane
street for jane street's kind of narrow
purposes and now there's been a really
deep collaboration between engineers at
jane street including jeremy dimino who
wrote the first version of it and runs
the team that manages it at james street
and collaborates very closely
with ocama labs and so both the kind of
industrial side of that work and the
open source side of that work
are well handled and handled by
different parts of what is essentially
one big team that's working on multiple
aspects of the problem that's right the
fundamental value that cambridge brings
is training mentoring and graduation so
it's graduation is a really important
part of cambridge where you leave and
you go do something else and the same is
true for indria and the universities in
france where the merlin developers came
from and i'm particularly proud of the
number of people that have learned and
moved on from cambridge to other jobs in
the ecosystem and succeeded so stephen
dolan and leah white both of whom are on
this podcast started off the degrees in
cambridge did their phds there and have
moved into jane street and many other
graduates have done similar as well
and it's crucial for the longevity of a
community to have this kind of easy flow
of people across jobs because obviously
people's lives change they can't just
all stay working in university and
cambridge was extraordinarily flexible
in figuring out how to get people in so
david also for example who is one of the
most prolific contributors to cora camel
is also a counter-tenor singer in his
spare time but when i say his spare time
is actually his career so i had to
convince him to come be a developer here
because he was working on a camel in the
spare time while also maintaining his
singing career he successfully juggled
both of those and became an incredible
contributor and an incredible singer but
explaining to cambridge hr exactly why i
was hiring a singer to work in my
research group was a challenge but they
didn't say no and he's still in the
camera labs and he's still one of the
prime maintainers many years on one of
the big and long-running projects that
ocam labs has taken on and really driven
is
the work towards having a multi-core
garbage collector furrow camel on a
multi-core capable of runtime
this is a long-running sore point about
o'camel you mentioned
one of the limitations in mirage is that
o'camel is not multi-core capable in
terms that you can't run multiple ocamel
threads that share the same heap this
has been a thing that people have talked
about for a very long time and there's
been some amount of work on and some
discussion about how to get there for
many years one question i have is why
has it taken so long why has this been
such a big and long running project to
add multi-core to the language
a really important part of research is
understanding that 90 of what we do is
fail and whenever we started adding
multi-core parallelism to camel we were
taking an existing
ecosystem an existing semantic for the
language and just trying to extend it
with the ability to run two things at
the same time instead of one thing
and the number of assumptions that break
when you do two things at the same time
instead of just one thing is incredible
so our first naive attempt was in 2013
we presented our confident plan for
exactly how multicore would go into a
camel and it got okayed by damien
delegates and sevilla then
a couple of years on we just realized
just how many edge cases there were and
the need for
a better conceptual core for what it
means to be multicore so we went to a
camel consortium meeting which was where
the industrial users of camel a few
years ago would present their needs and
requirements
and we presented our work to that team
and they said well look you can't add
this without having a memory model to a
camel so without a memory model which
says this is what happens when two
threads simultaneously access a single
or campbell value without that
definition it's really hard to ascribe
any meaning to multi-core camel because
what does a program do whenever this
situation happens so we then had to go
off for a year and figure out new
theorems and we came up with something
called ldrf local data race freedom
which was published in pldia a top tier
conference but it also crucially
resulted in addition to this nice new
theorem to a clean well-defined semantic
for a multi-core parallelism and a camel
so then we went back to the core
development team and we said hey here is
this clean memory model cementing it
went yeah great where's the rest of it
but remember there's only about two or
three of us working on this while
juggling many other things so we then
went off and frantically started riding
the garbage collector and making sure
that we could finish off the job
the garbage collector is more difficult
than a normal single threaded one
because it has to deal with multiple
cores simultaneously wanting to trigger
garbage collections and you have to make
sure that
irrespective of when the garbage
collection is happening that the program
is still maintaining type safety so
nothing can ever observably be violated
by a garbage collection happening and we
ended up with two separate schemes for
garbage collection and we couldn't
decide between them we then had to write
a full paper about this we had to make
sure that we evaluated both sides
and we also had to do this against a
backdrop where we could not tolerate
more than a few percent
of a performance hit for older camel
code
so if you were building a new language
you could just go ahead and build it and
you could build the perfect parallel
algorithm because you have no
compatibility to worry about but
meanwhile we had the entire proof
assistant community that said we're not
going to use multicore for a few years
but if we compile our existing code with
multi-core camel it shouldn't get any
slower and back then we'd had maybe a 10
or 20 percent performance it so a
significant
slowdown until you use multicore after a
few years of work we got that down to a
few percent so it was almost
indistinguishable from noise because all
of the various techniques that we put
into the garbage collector and the
compilation model to ensure that that
happened
this was again
real research so it got published in
icfp we then had to figure out how to
present this to the core development
team get consensus and then move it
forward
i think we have been working on
multi-core incrementally since of camel
4.0.2
so camel 402 was where we had the first
branch of our camel for multicore we're
now in a camel 413 which has just been
branched and i think in every version
since 408 we've put in a significant
chunk of work in order to get towards
multi-core parallelism most of these
things are invisible to their camel
users so you at jane street have been
using different parts of the multicore
compiler that we have upstreamed into
lots and lots of different versions of a
camel and we've done so in such a way
that it totally respects backwards
compatibility because
if you don't get it just right then
we'll end up with a split world where
the multi-or camel compiler is a new
language and it won't work with older
existing camel code and that would be a
disaster so the reason we're so careful
in threading the needle is that whenever
camel 5.0 lands it will compile almost
every bit of existing code in the last
25 years with a minimal performance hit
it will then allow you to add multi-core
parallelism through this domain's
interface
and it has one of the best and clean
memory models out of any language so our
research paper on bonding data races in
space and time
showed that
c plus plus and java the two kind of
gold standards for their memory models
have disastrous
issues
is the best way to put it so that's the
opening of our paper and we showed that
with just no performance at an x86 and a
two percent performance hit an arm and
power pc or sorry 0.4 on arm and and 2
on power pc we could make it all work so
that's a pretty big result it took a lot
of theoretical computer science a lot of
experimental evaluation and a lot of
implementation all all of these had to
happen simultaneously it wouldn't have
been possible without casey sivara
christian who's worked with me on this
project for the last six years
and we've gotten two top tier papers out
of it so it's not been a great ratio of
coding to papers but the end result is
something we're very very proud of so
the story you're telling highlights a
lot of the ways in which o'camel is
legitimately an academic language and
that part of the way of moving things
forward and convincing people to accept
a new feature
is actually going through the trouble of
writing serious academic papers to
really outline the design and explain
what the novel contributions are and
there are some novel contributions
so
from a kind of more ordinary
work at a systems programmer perspective
how should someone who is used to the
parallelism story in java
think about
the advances in o'camel how it from a
pragmatic point of view
is the coming o'camel multi-core runtime
going to be better um it's only going to
be better because it will not have any
surprises so whenever you use
multi-core parallelism in java you have
to know a lot of things you have to know
about the memory model in java you have
to understand
the atomics and the various interfaces
they're exposed there's different levels
of things exposed in different versions
of the jvm in ocamel this is potentially
just because of the young age of
multicore and camel
we think we just have a cleaner model
that avoids a lot of pitfalls that java
made now one of the interesting
properties about programming languages
is that it's very hard to take back a
semantic so if someone has written some
code in it there's just a vast number of
complaints if that changes because it
can fail at runtime so just by waiting
for this long and observing how all the
different languages have built their
systems and then doing the research to
thread that needle to find
the least surprising
memory model across all of the hardware
deployed today that's what i have in a
camel so a java programmer should find
it the most boring experience to do
multi-core parallelism in a camel
they'll just use high-level libraries
like domains lib that give them all of
the usual parallel programming libraries
and it'll just work
no surprises
fast do you have like a pithy example of
a pitfall in multi-core java that
doesn't exist in multi-core camel so
there's something called a data race and
when you have a data race this means the
two threads of parallel execution are
accessing the same memory at the same
time and at this point the program has
to decide
what the semantics are so in c plus plus
for example when you have a data race it
results in undefined behavior for the
rest of the program the program can do
anything conventionally daemons could
fly out of your nose is the example of
just what the compiler can do
in java you can have data races that are
bounded in time so the fact that you
change a value can mean later on in
execution because of the workings of the
jvm you can then have some kind of
undefined behavior so it's very hard to
debug because it's happening temporarily
across executions of multiple threads
in our camel
we guarantee that
the program is consistent and
sequentially consistent between data
races
it's hard to explain anymore without
showing you fragments of code but
conceptually
if there's a database on a camel code it
will not spread in other space or time
so in c plus plus if there's a data race
it'll spread through the rest of the
code base in java if there's a data race
it'll spread through potentially
multiple executions of that bit of code
in the future in ocam none of those
things happen the data race happens some
consequence exists in that particular
part of the code but it doesn't spread
to the program so if you're debugging it
you can spot your data race because it
happens in a very constrained part of
the application and that modularity is
obviously essential for any kind of
semantic reasoning about the program
right because you can't be looking in
your logging library for undefined
behavior when you're working on a
trading strategy or something else it's
it's got to be in your face at the point
yeah it seems to me like the core thing
you're talking about is
buggy code is easier to reason about
it's enormously important because almost
all code is buggy like parts of every
code base have bugs and problems and
this is why the classic undefined
behavior stance of
traditional c and c plus plus compilers
is so maddening because
there's a kind of amplification of error
where you make some mistake when you
step outside of the standard and
suddenly you know anything can happen
i've actually been seeing this happening
with my son who has a summer internship
where he's off hacking out a bunch of c
code and when you make a mistake in c
code it can be really hard to nail it
down
because the compiler can make all sorts
of assumptions and push the mistakes
into places where you totally wouldn't
expect it it sounds like the same thing
happens in the context of data races in
c and c plus and to some degree in java
and reducing that just makes it more
predictable and makes debugging easier
so that i feel pretty convinced by this
story it's quite pleasant working in
multicore camel when it comes to
debugging things because of this
property
so are you brave enough to venture a
date by which a mere mortal who installs
the latest version of o'camel will be
able to run two threads in parallel that
access the same heap well i can't give
you a date but i will give you well like
i i i can't give you did you can do that
today so what i did uh
so what i did a couple of weeks ago was
to merge the multi-core camel
working tree that we use which is a set
of patches against the latest table of
camel into the mainline op-amp
repository so this means with one line
you can switch from a camel 4.12.0 to a
camera 4.12.0 plus domains
and all the work that the multicore
camera team has been doing has been
focused around ecosystem compatibility
you can just start with your existing
projects and you can then start adding
in domain support and if you're really
really experimental we have a future
looking branch which also adds something
called an effect system on top of this
patch set this effect system is the
ability to
interpret
certain external events that happen
and just deal with them through what are
known as effect handlers so for example
if i'm writing to a blocking network
socket instead of having to then use
async await or lwt or monadic style
concurrency
our effect system just lets another part
of the accommodate program deal with the
blocking i o and then resume the threat
of execution whenever it's ready to
happen again so this is highly
experimental but it results in some of
the most pleasant and straight line on
camel core i've ever written it reminds
me of writing code in the early 2000s
when we just use p threads and unix for
everything all of these different
variants and levels of the camel
compiler are now available in opam
so depending on how near line features
you want to test all of the trees are
available for you to try out and the
next thing we're doing is that we're
working on camel 5.0 and this is
hopefully going to be the release after
413 which
contains the domains only patch set it
will expose
just
two extra modules that provide you with
the ability to launch multiple threads
of execution after six years of work
it's two modules
but those two modules obviously have
enormous power because you can then use
those to to spin up without having to
fork multiple processes or and do lots
of complicated serialization multiple
threads
and then our plan gets more experimental
5.0 is the sole focus of features that
we have been approved to get into coral
camel because they've gone through
extensive peer review
then for 5.1 our plan is to propose the
runtime parts of this effect system this
lets us not only express parallelism
which is what you get in camel 5.0 but
concurrency directly in the language so
the ability to interleave multiple
threads of control in a very natural way
this is a original research that we just
published in pldi this year on how we
made the runtime part of the effect
system as flexible as possible and again
without breaking any compatibility with
your existing tools so it uses gdb and
all of the familiar debugging tools
you're used to
and then later on at 5.2 we're going to
expose that effect system into the core
camel language using something known as
effect handlers and typed effect
handlers we're doing that in close
collaboration with james ford engineers
as well so this roadmap is multiple
years of work but the first step or
camel 5.0
we'll get into your hands as soon as we
can but
all the all the trees
all the trees are are in open source and
the way to speed it up is by giving it a
try trying your applications against it
and giving us bug reports so that's the
heart of open source and how you get a
concrete date help us to help you
question well dodged
[Laughter]
by the way just to
highlight a little point you said there
you mentioned how
the domains only version of it is meant
to provide the basic parallelism and
then on top of that you want to add some
notion of concurrency like in some sense
once you had parallelism there's some
amount of now concurrent execution but i
guess this reminds me of the old solaris
style you have some number of kernel
provided
truly in parallel threads and then you
have some kind of micro thread notion
that operates inside of there that's
lighter weight and that's the split
that's really being talked about here
like the idea is you have something like
one domain that you'd run per say
physical cpu that you have and then you
might have tens or hundreds or tens of
thousands of little micro threads that
are running inside each domain and
importantly
migratable so you can take one of these
and pick them up and move them to a
different core
so that's i think an important part of
that model it's a really important point
instead of calling them micro threads we
call them fibers so these are really
lightweight data structures you can have
millions of these in your heap resuming
them on a different core is just a
matter of writing some camel code the
really nice thing about effect handlers
is that your schedulers the things that
normally the operating system would
decide to do for you like thread
scheduling are written in ocamel as well
and so this means that you can write
application specific logic for things
that conventionally the kernel would
take care of for you and the colonel
doesn't really know how to do things
optimally he knows how to do things to
cause the least harm and so by this kind
of domain specialization your
applications in our camel can get really
really fast now this should be familiar
to you right because this is the future
of mirage os the goal of the effect
system is to internalize about a
decade's worth of learnings about how
to build portability libraries how to
build abstractions and device drivers
and now we're having the time of our
lives rebuilding all of these things in
direct style code using the effect
system so we have a new effects stack
called eio which is pure direct line
code its performance is competitive with
rust and go and so on i think it's
faster than go by quite a long way and
it's competitive with rust so
and it uses all of the new features in
operating systems i o urine and linux it
uses grand central dispatch in mac os
and ios and it uses iocp subsystem in
windows and all of these things happen
invisibly inside the io subsystem
written in camel but as a programmer you
just write normal straight line or camel
code and the effect system takes care of
all of that for you so it's a very very
exciting frontier for what's coming into
account in the future and it makes
mirage us code even more miragey because
it's just normal a couple code that you
write and all of this stuff is being
handled for in the background through
various effect handlers well i think
that's a fantastic place to stop as you
kind of tie a little bow around
connecting mirage and the most recent
work you've been doing in ocamel anil
thank you so much for joining me this
has been a real pleasure thanks ron fun
as always
all right cheers
you'll find a complete transcript of the
episode along with links to some of the
things that we discussed including
mirage and some of anil's other research
at signalsandthreads.com thanks for
joining us and see you next time
[Music]
you