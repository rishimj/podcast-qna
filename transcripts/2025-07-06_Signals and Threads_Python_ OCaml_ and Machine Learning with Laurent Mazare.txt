# Podcast Transcript
# Show: Signals and Threads
# Episode: Python, OCaml, and Machine Learning with Laurent Mazare
# YouTube URL: https://www.youtube.com/watch?v=d4SoT7rP28k
# Downloaded: 2025-07-19 20:58:00

[Music]
welcome to signals and threads in-depth
conversations about every layer of the
Tex stack from Jane Street I'm Ron
Minsky today we're going to have a
conversation about the use of python and
oaml at Jane Street with L maer Jan
street is pretty widely known for the
fact that we use OK camel which is a
statically typed functional programming
language similar to languages like Scala
and FP and Swift and hcll but the story
is more complicated than that we don't
only use oaml we use some other
languages too and one that we use that's
pretty important is Python and we mostly
use Python for data analysis and machine
learning tasks but it also extends
somewhat Beyond those domains so the
topic of the conversation this morning
is about how python fits in with o camel
and how it fits in to the broader
infrastructure at Jan Street one of the
reasons I'm excited to have this
conversation with L is both that he has
done a lot of work inside of Jan Street
both working with python and on making
the python tooling itself better but
also he has broader experience working
with different languages in different
contexts and so has some perspective on
this from more than just his time at Jam
street so lauron to start us off can you
tell us a little bit more about your
experience outside of jam Street and how
that led you to working here yeah so uh
my first work experience outside of
Academia was at a small company called
lexif where we were using okl to model
Financial contracts so in that context
okl was used mostly as a domain specific
language to represent complex Financial
payoffs I spent a couple years working
on the overall infrastructure that we
were selling to A Banks and assets
manager and after that I started working
in 2010 at Goldman Sachs in London as an
equity strategist there I was using the
inhouse programming language called
slang which one can think of as a
variant of python it's mostly a
scripting language it was fairly nice to
actually quickly see the output of what
you were doing but of course efficiency
was a bit of a concern I spent a couple
years working at Gman and in 2013 I
joined Jan Street as a software
developer I work mostly on trading
systems at Jan street so back to using
okl mostly everything there I enjoyed
functional programming aspects and
strong sty system when it comes to
design production critical stuff after
four years and something at Jane Street
I actually left to go work for Deep Mind
mostly because I was passionate about
machine learning at that point and I
wanted to work at bleeding edge of the
domain at Deep mine I use mostly python
to do machine learning with of course
tensor flow and I also got to use Swift
a bit as a replacement potentially for
python I spent a year working there and
then I came back working at Jam Street
where I focus more on Research heits
nowadays so I would say that I spend
half of my time working in okl and half
of my time working in spyon but these
two halfs are very different another
part of your background is you've also
done some interesting work on the open
source side can you say a little bit
about that so I tried to be active in
the open source community and through
that I had the opportunity to work on a
couple package the most notable one is
probably okl torch which provides okl
bindings for the py torch framework py
torch is this amazing thing developed by
Facebook that lets you write some deep
learning algorithm in Python and
leverage your GPU and the power of Auto
differentiation via these okl bindings
you can do that from okl to so you add
type safety to the mix I also worked on
Rust bindings for it so it's kind of the
same thing in this context
you have bindings in Rust so rather than
writing python code you end up writing a
rust I also worked a bit on clippy which
is the rust static
analyzer there you try to analyze rust
code and find legitimate errors and
there are plenty of other okl packages
that I looked at that can be bindings
for tensorflow bindings for Apachi Arrow
I tried writing a data frame Library for
no and a couple other things all of them
can be found on GitHub so this all
reflects you spent a bunch of time
working in a bunch of different places
using different languages and ecosystems
so you mentioned along the way that here
about half of your time is spent working
on Python and half is spent working in O
camel and those two halves are very
different maybe you can say a little bit
more how are those two different so yeah
the the first half of my time is spent
using Python and that's mostly for
research purpose in that context I will
use python in a notebook so for people
that wouldn't know what a notebook is
it's a kind of web UI where you have
some cells where you input some lines of
your programming language most of the
time Python and you can evaluate each
cells in order the development mode is
very interactive in this context so you
look at the actual output twe the your
code a bit then evaluate it again then
tweak your code again a bit because you
notice that you've made some MI stes and
start over again and again so it's some
kind of interactive development what is
fairly nice is that you have a very
quick feedback loop between editing the
code and seeing the actual outcome and
of course it plugs very nicely with the
very large Pyon ecosystem of plotting
libraries so you can actually run your
algorithm on some data and then plot the
output check that it match your
intuition if not try to debug TW your
code and start over so that would be
most what I use Python for and on the
other side there ISL and there it's far
more of development work and production
critical development I would say where
you tend to build systems that are
fairly resilient what I'm often Amazed
by is that you write some okl job uh you
kind of spend a week on it deploy it and
then you come back to it a few years
later and maybe even you've left and
come back along the way and you notice
that your job is actually still there
and still working which is pretty
amazing of course on the python side
it's a bit more rare than things will
keep on working for a very long period
of time say more about why that is what
do you think it is about o camel that
makes it easier for writing these kind
of more robust tasks for catching bugs
all of that yeah so I think there are
two aspects actually to it one is about
okl and the other one is about the
general engineering practice that we
have around the caml so it turns out
that when you write a camel code you
usually write proper tests for your
things you think a lot about all the
error cases Again part of it because the
type system will enforce that and each
function will tell you oh actually I
might return a result but I might also
return an error and you have to think
about what is this error and how you
want to handle it and also your your
goal is to build something that is a bit
resilient so you also are in a state of
mind where you think a lot about ker
cases whereas in the python world it's
more about runtime exceptions so you
don't spend much time thinking about
what the function actually outputs and
most of the time you use a library for
which you don't really know what the
function output it turns out that you
kind of guess it try it most of the time
it works and you go on with that you
just check kind of interactively that
it's what you would expect I'm really
struck by your description here there's
a mix of differences between the two
languages and some of them are
fundamental things about the language
some of them are things about the
ecosystem around the language and some
of them are issues about the practices
that Jan Street as an organization has
around the different languages so maybe
we could just dig into those somewhat
separately let's first talk about the
ecosystem what is it about the python
ecosystem and the O camel ecosystem that
make them different python I think
nowadays is kind of the de facto
standard for everything that is about
machine learning and data analysis so if
you want to be able to plot your results
again in a notebook you have plenty of
libraries that do that plenty of
tutorials online that will help you
finding the right way to do it and if
you run into any kind of issue again you
can just Google things you will get some
results super easily there is also a
wide variety of machine learning
libraries available all the major modern
deep learning Frameworks have python
front ends and are kind of released in
Python first many so the API that
developer focus on is the python one so
I don't think that there originally was
any very good reason for python to be
more successful than other language that
are a bit similar in that domain but it
turns out that the more you have an
ecosystem the more it attracts people
it's salls at some point on the other
side when it comes tol there are bunch
of libraries I think that in general
libraries tend to have a bit of a better
quality but you have far fewer of them
one way to get around this is that you
can bind to other languages so that's
what we did for tensor floor pych but
you can even bind to python so we
actually have some matte plotti bindings
when you want to plow something in okl
this will call python which itself will
use this very wellknown mat PL libraries
to render the plots and I think it's
really notable that the first thing you
talked about when you talked about the
ecosystem advantages of python is you
talked about visualization and I think
the visualization part is incredibly
important and I think fundamental to why
notebooks are such a valuable way of
working the basic mode in doing research
is exploratory you're going off and
trying something and looking at the
results and trying something else and
looking at the results again and having
a quick workflow that lets you do that
and lets you embed the visualizations in
your workflow in a straightforward way I
think that's one of the reasons why
notebooks are such a compelling way to
work and it's very different from the
traditional workflow the software
Engineers work which involves plain text
and text editors and just doesn't have
the same visualization component indeed
so I would say that these two mods are a
bit similar in a way that on both sides
you want to do fast iteration in one
case you will have this fast iteration
in the notebook so you will iterate
quickly over the data get some plout out
and get to actually visualize the output
of your algorithm check a bit some
Corner cases and iterate again and in ZL
work you tend to to do the same thing
about fast iteration but it's about type
safety so you uh save your file and your
build system will tell you oh this type
doesn't match this type doesn't match
this type doesn't match in the end you
would expect your Cod to pretty much
work once you've managed to get things
to compile whereas on the python side
you don't have these type aspects so
you're kind of relying more on
experiments to tell you if things are
working properly I wonder if there's
more of this experimental back in forth
on the o camel world then maybe you're
giving it credit for a place where I've
encountered this a lot is in the work
that we've done into building what are
called exp tests so expect tests which
is a thing that's not all that much
known outside of the jet context
although actually it was originally
stolen from an external idea the basic
idea actually came from the testing
tools that the Mercurial version control
system uses but anyway the basic idea of
expect test is you have this kind of
unified way of writing code and test
which in some ways feels a lot like a
notebook you write some piece of code
and then you write a special annotation
which means capture the output of the
code up till here and insert it into the
text of the code and then you write some
more code and have another place where
you capture output and then when the
test runs it regenerates all of the
output and if there are any changes it
says oh the output is Chang do you want
to accept the new version and then you
can hit a key in your editor and load
those new changes in and we use that a
lot for all sorts of different kinds of
testing but also for various kinds of
exploratory programming so for example a
thing that I've done using this is web
scraping there's some bit of web data
you want to grab and analyze and
transform and pull some information out
of and that's like very exploratory
right you're often not trying to get it
right in a kind of universal way for all
time you're trying to process some
particular kind of documents and so you
write some Transformations and you apply
them and you look at the output and see
if it makes sense and the build system
actually gives you a relatively good and
fast loop where you go and you make a
change and the build completes and you
get to see the new output one of the big
differences there's no graph component
everything is plain text one of the big
advantages you get out of the notebook
style workflow is the ability to be in a
web page and actually display graphical
stuff that's right I kind of agree that
expect tests give you some kind of
interactive experiment there I think you
focus more on Corner cases and hard bits
for your algorithm whereas most of the
time on the notebook experiment in
Python you're not writing some code that
is there for a very very long time so
you actually want it to work on your
data rather than on any potential data
so even if expect tests are a bit
interactive I think that still a bit of
a different experiment so I'm curious to
what degree you think this is an issue
of the programming language or an issue
of the tools and ecosystem around the
language if for a minute we imagined a
world where the tooling for using o
camel in a notebook was all really well
worked out you had all of the nice
things you're used to from the editor
into ation experience we have with a
language like o camel so like you can
navigate to a piece of code and look up
the type of a given expression or
navigate to the definition of a given
value all of those things all of those
IDE like features worked really well and
at the same time you had all the
benefits of using python in a notebook
in terms of the interactivity and the
ability to visualize things and you had
all of the machine learning and data
analysis libraries at your disposal at
that point is there still a reason that
you'd want to use Python is there
something about the language itself that
makes it a better tool for some of this
kind of exploratory research work yeah
it's a very interesting question it's of
course a bit hard to answer definitely
because you don't know what it would
look like if you had everything possible
on the okl side still I feel a bit that
the dynamic aspects of python have some
advantages that you will not have on the
okl side a typical thing that you will
do in Python is you write your small
algorithms it actually works for
integers and you're happy with it but it
turns out that later you actually want
to use floats the fact that in okl to
make the type system happy you would
have to modify all the operation will be
kind of annoying to you whereas in
Python you would expect things to work
most of the time you will still have
this problem that if there are some
errors you might not detect them but on
most reasonable output as the checks
that you've done should bring you some
confidence that it's working reasonably
well and all these Dynamic aspects are
fairly nice it lets you try pretty much
any kind of algorithm on any data and
relying on duct typing as long as your
object implements the proper method Sy
will will work out without you actually
knowing what's taking place behind the
hood I guess one of the big upsides of
working in a language with a static type
system is that types provide a lot of
guarantees guarantees that you can't get
from any single run of the program you
can't get by any amount of testing
because the guarantees of type systems
are essentially Universal they apply to
every possible execution of your program
so that's an incredibly powerful thing
but it comes at a cost getting those
guarantees requires you to effectively
fit your program in to the structure
required by the type system like type
systems are good at capturing some kinds
of rules and not good at capturing
others so there's essentially a kind of
negotiation step where you figure out
how how to play nicely with the type
system and in a language like o camel
that actually works out pretty nicely
the type system is quite lightweight it
doesn't require lots of type annotations
and it's flexible so you you don't have
to bend your program too far out of
shape to make it fit in and to get the
benefits of types but the nice thing
about dynamically typed languages is
that you get to skip that negotiation
step entirely if your program works in
the particular context on the data that
you happen to be running on then you
don't have to think about how it works
in a more General context and when
you're doing EXP latory research writing
little bits of code that you get
information from and then maybe never
run again I guess the trade-offs are
really different at that point it being
able to operate without types has a lot
of appeal yeah so the type system in OK
will indeed give you a proof that your
code is correct and that the code is
correct for all possible inputs but in
Python it's kind of the default that
things you don't get any proof that your
code will run correctly you will only
get exceptions at run time turns out
that the fragments of input that you're
caring about might be very small so the
time that you would spend looking for
Corner cases on the oaml side and fixing
them is actually not necessary on the
python side because yourself you know
that you won't have that kind of inputs
whereas of course if you have a typer
and a compiler to ensure type safety the
compiler cannot assume anything about
that so you will have to teach it
actually I don't really care about this
variant nor about this variant and so on
I feel like we should be a little less
Grand in the sense that it's not that
the type system gives you a proof of the
overall correctness of the program but
it does capture certain elements of
correctness and Nails them down pretty
well and there's certain kinds of
programs where that gets you alarmingly
far where large classes of bugs totally
go away under the scrutiny of the type
system the thing that's also maybe worth
mentioning is that type systems like o
camels actually do relatively little to
capture bug's numerical code one of the
hard things about numerical programs was
when you get the algorithm wrong things
look a little worse in fuzzier ways if
you didn't do your linear algebra
decomposition just right then it's not
that you get a totally crazy answer you
get like an answer that's less optimal
or it doesn't converge as fast or it
doesn't converge at all and separating
out those bugs finding those bugs is
actually quite hard and I suspect if OK
Camel had a type system that was really
good at finding that kind of bug people
would be really eager to use it in data
analysis contexts but in practice the
hardest bugs in numerical analysis often
just kind of slip by without the type
system helping you much at all yeah so
indeed the bread and butter of python is
numerical algorithms and in that case
pretty much everything is a float or an
array of float or a matrix of float so
having a type system that checks that
the type are correct might not buy you
that much as you mentioned the problems
are more likely to be the float value is
not the correct one rather than the
float is not a float but it's actually a
string so in that context the Y type
system definitely helps less you
mentioned that you might want some
specific type system to catch that kind
of error but that turns out to be quite
tricky even being able to decide if a
float is going to be a nan or is going
to be an actual float is not a
straightforward problem and yeah that's
the kind of thing that by experiment you
will actually catch by looking at the
data that you're taking as input and
trying your algorithm but if you have to
handle every possible data you probably
don't want to check for floats at any
point in your function it would be too
much of a pain right and I feel like
machine learning is split between two
kinds of work some of which is all about
fast iteration and some of which is all
about painfully slow iteration which is
to say some of the work you do is taking
some big model that you want to train
and shipping it off to a farm of
computers and gpus and such to do a
bunch of heavy lifting and then get the
results back at the end and that seems
like case where you would really really
like to not discover that after like 3
days of chomping on your numbers that
some trivial bug at the end causes the
whole thing to fail and you realize you
were just wrong and so that feels like a
case where you'd actually rather have
more checking upfront if you could get
it is that like a thing that comes up
and is that something where you think o
camel can be useful in a machine
learning context yeah I think that the
the kind of place where Roa could be
nice turns out that what would you tend
to do in python is you run your model
first with very small input sizes so
that it runs quickly and you run the
whole thing and you check that it's able
to write the model file in the end or to
write some intermediary snapshot files
but you can still miss some branching in
your code or whatnot and so you don't
see that at the end your model will
explode and you might just not get
anything out of multiple hours SL days
of comput and in that case you're pretty
sad so having a type system that tells
you oh actually here you said that the
file name was supposed to be a string it
turns out that you've given me an INT
somehow it's pretty good to be able to
detect that kind of failure then it
raise a bit the question of what is the
ideal system maybe the actual model
should be in Python because it's a
numerical bit and you want to iterate
quickly about building this model but
once it's come to more either production
usage or training for real which is kind
of a production thing then you want an
actual infrastructure around it that is
resiliant about lots of thing so about
your some computer being done in your
compute Farm about the file system
having issues and of course about silly
mistakes that you might have had in your
code are you suggesting in this world
you would keep the modeling in Python
the whole way through but just build
infrastructure around it in OK because
taking the thing you wrote in Python and
rewriting it again in okam doesn't sound
like an enormous amount of fun yeah and
you might have discrepancies between the
two things I think it's a bit clearer
even when it comes to productionizing
things so you write your model in Python
and even if you train it in Python at
some point you're probably going to want
to use it on real life things for Jan
Street it will mean that you might have
a trading system that is actually using
your your thing and this trading system
is likely to be written in a Cel because
it has to be resilient to lots of
different things and still you will want
to interface with your model at some
point so you have multiple ways around
that you can either make okl call Python
and use that to decide on the model
output take the value back and process
things accordingly but you could also uh
just try to replicate everything so that
things might be faster you might have
more type guarantees but of course you
have two different implementations so
then you have a new problem which is how
do you Ensure that the two
implementations stay in line over time
it's not something great to have but
we've done it for a couple things and
one way we have around that is when you
produce a model the model outputs itself
like the configuration of Weights that
you've finally reached as well as some
sample values and on the okl side when
you deploy the actual model you will run
with the configure weights and the
sample values and check that you obtain
the exact same results so some of what
you were talking about there had to do
with the tooling that we have for
calling back and forth between Python
and O camel and that's actually an area
that you've done a bunch of work on so
can you say a little bit more about what
we've done to make interoperability
between the two languages better yeah F
that we are we rely a lot on a library
which is called PML which is open source
and which is very neat uh this Library
it's mostly design so that you can call
python from AME so again a typical
example is you want a plotting library
in okl so you want to use M plot clip
because it has its own issues but it has
tons of features and it works fairly
well so you just call the python runtime
from your okl runtime but it turns out
that you can also use it the other way
around and it's the kind of use case
that we have the most of at Jane Street
when you write things in your python
notebook you want to be able to access
the various chain street services to get
the market data to get actually all the
data that you can think of in your
python notebook and even to trigger some
actions or to publish some values to our
different system so for that it's more
calling okl from Python and again with
PML what you can do is compile yourl
code in a shared library and this shared
Library you will load it from the python
runtime you will call the starting point
of the okl code which will bring up the
okl runtime and the oaml runtime will
register lots of functions for the
python runtime to be able to use this
makes it possible to actually write
python wrappers around okl function in a
way where you almost don't have to write
a single line of python so for people
that tend not to use Python that much
that's very nice and also when you want
to build functions that are used a lot
by lots of different people on in that
you might not have think of you're
actually pretty happy to do that in the
okl world rather than to do it in the
python world and the key reason it's
important to do this without writing any
python is there are lots of oaml
developers who are developing libraries
that would be useful to python
programmers who themselves basically
don't use Python at all and so you want
to keep down the amount of work that's
required for non-python developers to
export things and make it available to
the python world because if people don't
run the python command line to test
things or to write small rappers on a
daily basis and they might not remember
how to do it it's a bit of a pain
whereas if you do everything on the okl
side people in the development area are
very familiar with doing that great and
it it's may be worth noting this is the
kind of thing that we've done in
multiple different places what you're
describing is the ordinary thing that
you have in any kind of higher order
language whether that's a functional
programming language or an
objectoriented language where you're
essentially shipping around pointers to
bits of code and it's pretty common to
have things which when you stop and
think about them they're just calling
back and forth multiple times across
these different levels and then what's
interesting about it in a python context
is we're now calling back and forth
between two different language run times
not just between two different functions
in the same language and we do exactly
this for our oaml emac integration with
emac we wanted to write lots of
extensions but we really didn't want to
write them in the native language that
that emac uses for this called elisp
because people found it harder to test
harder to teach and harder to engineer
and so we wrote this layer called eaml
and now pretty much all of our emac
extensions are written in oaml but you
still need to interrupt with the basic
emac functionality in library so in that
sense it's very similar to the python
story and you have exactly the same
story where you have programs that are
constantly bouncing back in fourth
between oaml and between emac lisp in
that case and hilariously we're now
doing the same thing with Vim so you
have the same kind of Technology getting
built over and over yeah it's pretty
amazing when you build that kind of
system and you notice the first time
that it works it sounds pretty weird
that somehow you're able to interface
that having layers of python calling
oaml calling python calling oaml and
somehow even if you do that hundreds of
time it ends up just working so there
are some nasty aspects to it while doing
the integration of some okl function we
actually run into a pretty funny bug
because of that which was as follows so
uh okl as a garbage collector so the okl
run time from time to time stop the
world and collects what is not alive
here by alive we mean the value that can
be reach by their on time so it's all
the values that are actually useful to
the user still at that point and all the
rest is the garbage that can be
collected and removed whereas on the
python world you use reference counting
so on each object you keep a counter of
how many times this is used it has a
nice benefit that you can release things
as soon as possible but it has a
slightly sad aspect which is if you have
a cycle then memory is lost so you still
need a garbage collector that you run
from time to time to detect cycles and
to actually remove them and the bug that
we actually encountered was because you
had a cycle between the Python and the
okl run time so you had an okl object
that was itself pointing at python value
and the python value was pointing back
to the oaml thing and that thing cannot
be detected by either of the garbage
collector each of them will tell you oh
actually that thing is being used I
don't want to remove it but overall you
could remove the old value and it turns
out that this was wasting tens of
megabyte because of that and so we
noticed it pretty quickly and fix the
thing but there are some small drawbacks
that you can have because of this this
is exactly the classic problem you get
when you have two garbage collectors
interacting and we have exactly the same
bug in the E camel story so it's I think
it is a fundamental problem that's hard
to fix so I want to kind of go back to
this question of how do we think about
Python and EML and to ask you when
you're going off to engage in some
programming task how do you think about
the choice between whether you want to
use Python and whether you want to use
oaml myself I would think that when
things are there to last and it's some
cod that you want to still be working in
multiple weeks or months then okam is
very neat and also there are specific
domains where OK shines when it's about
manipulating symbolic values of course
trying to do that in Python would
probably not be that much of a good idea
one good example of manipulating
symbolic values is writing a syntax
extension in that case okl type system
does a really good job of helping catch
errors in codes that's going to inspect
your program and generate new code
depending on what it finds and making
sure that you don't forget to under all
the different kinds of syntax that show
up in the language although I say that
the kind of example you gave about
writing programs that manipulate
programs is both a very good example in
the sense that it highlights something o
camel is really good at but also I feel
like in some ways a bad example because
I think it understates how often this
kind of programming comes up to try and
frame it in a somewhat more General
context I feel like the kind of places
where a camel works really well is where
you have combinatorial structure that
involves just differentiating between
lots of different cases right if like
there's different ways that your data
might be laid out and you want something
that helps you do the case analysis and
make sure that you capture all the cases
okl is incredibly good at that like
emel's pattern matching facility gives
you a way of just exhaustedly saying
what are all the different ways this
data might be shaped and making sure
that you cover them all and that for
sure shows up and shows up a lot when
you're doing compiler style work or
generally things that kind of feel more
like program manipulation but it
actually shows a lot in various kinds of
systems programming tasks like I've
spent a lot of time working on the
insides of trading systems for example
and there's actually lots of places
where you want to think in exactly this
kind of combinatorial way and where
things like o camel do a really good job
of catching those bugs yeah yeah it's
definitely okay so you mentioned tra
trading systems and indeed it's quite
challenging building trading systems in
Python is not the best idea you were
talking before about how we built tools
to make it possible to call into OK
camel from Python and the kind of
examples you pointed to are cases where
you want to consume data where the
primary wrapper of that data is some bit
of ok camel code or to publish data
again through some path that's in oaml
but I think it's also useful to be able
to invoke computations pure computations
that exist on the oaml side I think for
two reasons one is because it's way more
efficient as you've been highlighting
but also because writing the code inside
of our ecosystem writing the code in
oaml means you could write the program
once write the computation once and then
share it on both sides being able to
have all of the core computational stuff
available on the oaml side and then
being able to expose it in Python is a
nice way to allow you to stop repeating
yourself and having to rewrite things on
both sides yeah efficiency is definitely
something that we will care about and
that would be far better on the okl side
and also if some code is indeed to be
called by lots of different people and
again these people might have ways to
call it that you will not have thought
about in Python it will be pretty
challenging because your test suit will
have to cover pretty much every single
Corner case and still people might at
some point rely on some specificities of
your code wrap around that which ends up
being quite a mess you don't want to
write in a Pon some codes that is going
to be queried in lots of uh different
usage scenarios whereas in AME you're
kind of forced by the type system to
think about all these different
scenarios and you want to get that
correct and it's not worth the
investment for a oneof but if it's some
code that is going to be shared across
tons of people it's probably worth more
of the investment when people think
about the difference between dynamically
typed languages and statically typed
languages they often think of well in
statically typed languages you have the
type system to help you and in
dynamically typed languages you have to
write a lot of tests which is I think
not quite the right way to think about
it I think in a statically typed
language you still need tests but they
have almost a kind of snap in place
property which is if you write your
program and can show that it works on a
few key cases and catch a few of the
corners then the overall Behavior tends
to snap into place and you get in so
almost like wider coverage of your tests
by having the type system make the
behavior of the program in some sense
more rigid so that you can just get by
with a much lighter testing story so you
still need to do tests but you don't
have to be nearly as exhaustive in the
testing as you do in a language where
tests are the only thing that are
nailing the behavior VI of your program
in place so earlier on when we were
talking about the trade-offs between
okel and python you talked about how
well on on the okel side we have all
this Rich testing infrastructure and on
the python side you know there's much
less of that and less practice around
that so first of all I imagine some
professional python developer listening
to this conversation and saying what is
wrong with you people like obviously
when you write python code you should
have good testing infrastructure I'm
wondering if there's stuff that we
should be doing and maybe are in the
process of doing to improve the tooling
story that we have internally around
Python and to make more of a culture of
testing the things that we do right yeah
we're certainly learning a lot on the
python side on the okl side things have
been Polished by the years and are very
very efficient on the python side we are
testing having a bit of type annotation
generating automatically the
documentation and so on and you have to
pretty much redo all the things that
have been done fora at Jan street so
we're going through that and trying to
focus first on the most important bits
but we're certainly not there and I can
imagine that our best practices are
pretty far away from people that are
doing lots of python I also think that
uh when it comes to actually using
python not alone to build your system
but more as some kind of glue around
some okl components the testing story
probably has to be less involved on the
um python side you still want your okl
components to be well tested you still
want your intermediary layer that
converts python calls into okl calls to
be of course properly tested but uh you
have far less of a possibility of bugs
on the python side because what we ship
on the python side is fairly limited
after that the user is going to write
tens of lines perhaps a bit more around
it but hopefully not that much that
there would be tons of possibilities for
bugs though we're quite commonly
surprised about how easy it is to sneak
some bugs in a very very short amount of
good that is a thing about which
programmers will never stop being
surprised yeah it's pretty impressive
and it's also impressive when you when
you start to rely on python libraries so
of course we rely a lot on something
called nire to represent
multi-dimensional arrays and also on a
data frame Library called pandas that
lets you represent Matrix of each colon
as an heterogeneous type like you can
have a colon of string a colon of time a
colon of a float and this library is
super powerful it works very nicely it
lets you analyze your data very quickly
but it has very weird Corner cases and
you might just notice that the code is
actually not doing what you would expect
and learn it the hard way because the
only thing you noticed in the end is oh
the error that I get while running my
simulation is not what I would have Ed
it's pretty bad so now I have to dig
down in my Cod annotate things quite a
lot and trying to understand why the
input and output of this pandas library
is not what I would expect to finally uh
Google that and discover that it's a
well-known gcha around the library and
that's only for the case that we know
about where the error was large enough
for us to discover of course you have
cases where you won't even know about it
this reminds me a little bit of what
data analysis is like in Excel Excel is
something like the equivalent of the
Python notebook the program that you
write in there is a mix of the little
expression language that goes into the
cells of a spreadsheet and VBA that you
write for manipulating and doing various
things that don't look like simple
computations that go along with it and
again it has all the benefits that we
were talking about which is it gives you
some ability to visualize the data the
this cell structured way of looking at
computations is actually in some ways
incredibly powerful you look at someone
who's good at a spreadsheet they're able
to very quickly do all sorts of rich
computations in a way where they can see
all the intermediate pieces and in the
transformation of the numbers become
actually much easier to follow through
because all the intermediate computation
is just kind of laid out in a Big Grid
and also it has a bunch of baked in
totally terrifying weird behaviors I'm
reminded of this recent news where some
organization in the academic genomics
Community changed the names they used
for various objects that show up in
spreadsheets because I think some of
them were being interpreted as dates
which caused all sorts of crazy things
to happen in the middle of spreadsheets
another example that we worried about a
bunch when it came out is a stock called
true and there was a bunch of worries of
like what would because Excel does a
bunch of stuff where again related to
the fact that it's dynamically typed
where it tries to infer what kind of
data you have from what it is that you
typed in and this is incredibly helpful
in lots of context and also absolutely
terrifying in other context and because
the work that's being done is numerical
when it fails again it fails in this
kind of soft way a lot of the time where
like the numbers aren't quite right and
you may just not notice and your result
might be a little less optimal than it
should be or give you not quite the real
answer that's the thing that we worry
about and are trying to be pretty
careful about and defensive about when
we write spreadsheets and there's stuff
in the outside world where well-known
results had to be retracted because of
subtle bugs in spreadsheets uh myself I
tend to make a lot of fun about Excel
because I live in Linux land I don't
like logging on a Windows computer I
don't use Excel almost at all but still
when you go and see uh when you have a
question for someone working on trading
at Chain Street and you see him
manipulating Excel it's pretty
impressive because people tend to know
all the shortcuts and are able very very
quickly to actually get their data out
of Bloomberg plot them in Excel
basically check that the values are in
line with what they would expect and
just send them to you whereas if you
were to have written okl cut for that
you would probably have spent multiple
hours on it so yeah Excel definitely has
its Advantage it also has a great uh
reactive model like having this cells
where when where you modify a cell
everything gets recomputed that's fairly
nice python is definitely kind of in the
middle you're able to do more uh
computation there it's a bit more
efficient and can handle larger data
sets but you lose a bit of the oh I can
basically eyeball all the data at once
so of course in Python at some point
what you're manipulating is too big so
you only look at statistics and
extremums that kind of things so it's
far less good than what it would be when
using Excel also the python notebooks
lack a nice property that Excel
spreadsheets have where in Excel
spreadsheets every cell is a computation
which makes references to other cell and
Excel keeps track of this graph of
computations and how they depend on each
other and that's useful both for
performance reasons right this this kind
of incremental update if I change
something it will refresh the
computation and just do the minimum that
it needs to do to refresh things so it
has a nice built-in incremental
Computing model but it's also good for
correctness reasons it can actually
refresh all the things that need to be
refreshed whenever anything changes so
you know that your spreadsheet is in a
consistent State and not so with a
python notebook in a python notebook you
you have something similar right you
have like little chunks of code which
are almost like the equations and then
you have various visualizations that are
interspersed between them and these
chunks of code depend on each other but
you have little buttons you can press to
decide which ones to rerun when and so
you can look at a notebook and you have
no guarantee that it's in a consistent
State and so that's a thing that's
always struck me as kind of
nerve-wracking about this model of
programming Excel tends to be more
functional than what python would be you
have far less of a notion of State than
can be mutated it's just this graph of
computation and that's actually fairly
needs the big problem with notebook is
what you mentioned of you can run each
cell cell may depend on the current
state and it's very easy to create a
notebook State and not remember how you
actually reach side state you might have
executed a cell multiple times and you
might have executed the third cells
before executed the second one and you
would have to redo everything again if
you wanted to be able to uh go back to
the exact same same state so for
efficiency reason you don't want to uh
keep this big graph and rerun everything
but it's kind of a problem because when
you are when you reach some conclusion
from your notebook if you're not able to
restart from scratch and re reach the
same conclusion easily it's actually
annoying is there any work in the
direction of trying to solve this
problem with notebooks to make it so you
could have notebooks that had more
natural incremental model so that you
didn't have to choose between efficiency
on one side and correctness on the other
pick one is there any work to try and
make that better yeah I think there is a
bit of work but nothing that has the
level of adoption of the main Jupiter
IPython thing so things that can be
mentioned there is an alternative to
Jupiter which is called the poly note
which is made by Netflix I think it has
the same kind of drawbacks except that
it's far more explicit about the notion
of state so at least you would see the
variables that your thing Define and
when you go back to a cell it tries to
put you back in the state that you had
when editing this cell so only taking
into account what was there previously
but of course because you want to do
that efficiently you don't really handle
aling correctly so if you're doing deep
mutation inside the object I don't think
that this is tracked done properly just
kind of the first layer of object is as
it was at that point in time I see so it
tries to bring you back but it's not a
sound method it's not guaranteed to
always work yeah because it would
explode probably right besides that
there is a a nice thing but it's very
experimental at this stage in the Julia
World which is called Pluto so it will
notice all the dependencies and whenever
you edit a cell it will recompute all
the cells that are depending on the
results so this works I think only for
Julia at this stage but it's pretty neat
it might be the case that you want to
rerun a cell and not rerun long uh
things that would be too long to run so
it's a bit of a user interface problem
at some point yeah although I guess you
could just be explicit about you could
say I rerun the cell and now if I could
keep track and Visually notify the user
that the following cells are in some
sense out ofd at least if you understood
and could expose to the user so they
knew which part of the computations were
reliable and which parts were not
reliable that would already move you a
big step forward you mentioned at the
beginning that you had spent some time
uh when you were at Deep mind working on
Swift as an alternative language for
doing machine learning work and the way
I understand it part of the story there
I think is about Auto
differentiation is that I guess maybe
you could just say a little bit more
about what the ideas are behind using
Swift as a language for doing this kind
of machine learning work so yeah the the
idea there is indeed differentiable
programming so what you tend to do a lot
in modern machine learning is doing
gradient descent so you you have a
function that you want to optimize so it
has some parameters and you want to find
Optimal values for these parameters you
have a notion of loss and you want to
find the parameters that minimize his L
and this the way to do that is by
gradient descent so you would just use
the Newton method of computing the
derivative except that in this case the
derivative is with respect to lots of
different variables and you follow the
slope down until you think you have
reached the minimum so of course what
you discover is a local minimum right
and for people who don't spend a lot of
time doing machine learning the the way
I always think about this is you have
some function you want to evaluate think
of it as like a series of Hills or
whatever ever and you know gradient
descent is just the thing that a rolling
ball does it just goes in the steepest
Direction instead of being at the top of
a hill you'll find yourself at some
thing that looks like a bowl in its
shape and that's exactly a local Minima
and having the ability to compute the
derivative particularly in a very high
dimensional context where instead of
being you know a two-dimensional picture
like the one I have in my head you have
200 Dimensions or 2,000 Dimensions or
more having that derivative there is
very a very powerful thing if you were
to compute the gradient numerically that
would work with a couple Dimensions but
if you have a model that has millions of
parameters you don't want to compute the
gradient numerically so having symbolic
gradient uh is far better and that's
where differential programming is
actually very helpful so you you can
either use uh a library like tensorflow
or py torch this Library will build a
graph of the computation for you and do
the derivation like the symbolic
derivation of this computation graph but
you can imagine pushing that that's one
step further and just doing that at the
compiler level so at this point where
you define a function that take as input
lots of uh float values and return the
float you can imagine that automatically
you generate the gradient for this
function because as a compiler you know
that this function is combining the
addition with some subtraction and lots
of other functions for which you might
have already comput it to gradient to
maybe make this more concrete for people
the notion of symbolic differentiation
is basically the kind of differentiation
you ran into in Calculus class you know
there were a bunch of rules that you
could apply you write down some
expression and there are rules of like
you know the derivative of x s is 2X and
you know rules about multiplication and
addition and composition and it turns
out this actually very old insight about
programming languages so-called
automatic differentiation is 30 years
old it showed up in lots of languages in
the past including ancient
implementations in Fortran and lisp
gives you a way of just saying oh let's
just take this idea and generalize it
and apply it to programs not just
programs that represent simple
Expressions but programs that do
iteration and recursion and all sorts of
crazy stuff yep and of course there are
a lots of challenges around the way
because it's easy to differentiate an
addition multiplication and the
composition of multiple functions but
you have functions that are far harder
to differentiate you mention oh I call
my function recursively or I might have
an if Branch things that are kind of
discrete rather than continuous and then
you have to decide about what is the
proper differ iation for that so it
turns out that modern machine learning
based models combine functions that are
all reasonably easy to differentiate so
if you just write uh a model in a
programming language that support
automatic differentiation you will
actually not need the library to compute
the gradient for you the compiler will
return you the gradient because you only
Ed operations that were easy to
differentiate and being able to compose
that for PR much all the function in
your program that might yeah that might
let you optimize things that you didn't
think you would be able to to optimize
so the work you were talking about is
trying to take this idea of automatic
differentiation and apply it to Swift
can you say a little bit more about what
kind of language Swift is and why people
looked at Swift as the language for
doing this yeah so so Swift it's a
programming language that originated
from Apple as a replacement for
objective c and so the person that
worked on it was Chris lner working at
Google he focused on uh what could we do
with modern programming language theory
in the machine learning world and it
turns out he tried a few languages he
tried what can I do with uh Swift what
can I do with ascal what can I do with
rust what can I do with Julia and it
ended up being Swift that was selected
to build a prototype which was probably
a good choice because he was leading the
project and was very familiar with it so
Swift is a a compiled language it has
some very nice functional aspects so you
have the equivalent of type classes
except that it's called I think
protocols there and overall it feels
like you have some types it feels like
uh a mod programming language the sense
I get is a mashup between something like
Objective C and something in the hascal
O camel FP kind of world right I think
The Objective C stuff was kind of just
needed fundament to be compatible with
the whole iOS world yeah I think that
the that's indeed the case there is this
I want to be able to be attractive to uh
Objective C users and to interact with
Objective C systems but besides that it
has lots of modern features and feels
quite close actually to okl when it
comes to the type system and yeah Swift
is compiled so it compiles down the llvm
and there are the the projects that
people were looking at and are still
looking at
at is uh let's take some Swift Code and
rather than compiling it for a CPU let's
compile it for different Hardware let's
compile it for uh a GPU so we know that
in this uh Swift Cod that I've written I
have some big matrix multiplication
addition and so on and I don't want to
actually Target a CPU for that I want to
Target a GPU or even this fancy ppus
that you can rent from Google online
line uh so you want the compiler to
actually extract the part of the code
that will run on your CPU extract the
part of the code that can run on a GPU
or on a TPU and do the actual data
Transit from some part of my system like
the main memory to the GPU memory or to
the TPU memory so that's actually a
challenging bit and the other aspect is
having automatic differentiation so
there you you want again to be able to
compute the gradients and of course you
want also that to be handled on the GPU
or on the TPU or whatever your Hardware
is so it's up to the compiler uh to
decide you just annotate a function and
say this function let's produce the
backward pass for this function roughly
its gradient and the compiler will
happily create at compile time the
gradient function provided that all the
functions that you use underneath and
all the constructions that you that you
do are compatible with that is there a
reason that Swift is a better language
to try this in than python python is
again a very dyamic and very stateful
Swift is closer as we said before to okl
and to something functional so you have
less of a notion of a state you tend to
have more pure functions and Computing
derivatives of course will make more
sense in a world of pure function than
it would if you start having some state
also the compiler has far more
information about what the function does
it doesn't mean that it's not possible
to do in Python actually a framework
like pytorch lets you annotate python
functions with just in time informations
that tells the pytorch framework to try
to compile the function and compile its
backward pass from the actual python
representation of the syntax tree one
advantage that also Swift has it's not
with respect to python but it's with
respect to okl is that Swift actually
uses reference counting rather than a GC
and that turns out to be actually pretty
important because when you're training a
big machine learning model one of the
challenge is to freeze the memory as
soon as possible your model is
allocating huge Matrix or tensors in
General on the gpus they can take
multiple megabyte or even gab and your
GPU only have I don't know 24 perhaps 48
GB of memory if you have a very very
fancy GPU so being able to release the
memory as soon as possible is something
very much worthwhile and that also
explain the success of python when it
comes to machine learning the fact that
it does reference counting helps with
that that also explain why Swift is
actually a bit better than okam to do
that kind of work at the moment it's
also explain the same thing for ascal so
I heard that ascal was getting linear
types and linear types is another way to
collect the resources as soon as you can
yeah that's right in fact we we're
looking at doing very similar things in
oaml and we've actually shockingly
enough hired someone where a big part of
their goal is trying to get the the work
on algebraic effects to not to go into
too much but algebraic effects is
another kind of type system level
feature which among other things will
make it possible on top of it to build
things that look more or less like
management of resources and in many ways
let you capture some of the same
Resource Management that languages like
rust have and I think this goes back to
the basic fact that garbage collection
is great for managing memory and
managing the one big resource to the
shared memory that your whole program
uses but garbage collection is a
terrible way of managing other kinds of
resources a classic example that you
don't need to think about machine
learning to understand is file handles
sometimes people will set up their
programming language so that their files
will get closed when the garbage
collector gets rid of the file but
that's terrible because it turns out you
have some shockingly low limit on the
number of open file scriptors you can
have and your program will just crash
because suddenly you can't open files
anymore because your garbage collector
didn't feel like it had to work that
hard to collect memory and therefore it
wouldn't collect this other completely
different resource of file handles so
for things like file handles or you know
GPU memory or various other kinds of
external resources that aren't just like
another chunk of memory on the Heap you
really want something else yeah yeah I I
feel that it would be very amazing to
have that kind of possibility in the okl
world at some point and like every
scarce resource indeed you don't really
want the GC to be accountable for it and
you want it to be grabbed as eagerly as
possible so I mentioned that I worked on
some pie torch binding for both rust and
okl and of course the rust one is very
nice because uh you get this all this
borrow cheer uh Magic that will ensure
that your resur like that your tensors
are released as soon as can be whereas
on the okl side most of the code that
I'm writing uses by torch I force the GC
to trigger on every training Loop
because I want the resources to be
collected so being able to mention that
for some variables you want them to be
track in a more accurate way is
something that would be uh fairly neaty
it's obviously ongoing work but I'm
pretty excited about having aam will be
a system where by default you have a
garbage collector but that you can in a
focused way where you want to do precise
Resource Management have it checked and
enforced at the level of the type system
and my hope is that that will end up
being a system which gives you a lot of
the things that are most attractive
about rust but is all in more ergonomic
because you don't have to do the extra
work of thinking about explicit tracking
of the memory except in the cases where
you really need to do it but obviously
this is like future stuff things we're
hoping to get to but it's all vaporware
at this point having a world where the
default is actually you have a reference
counting or you have a GC but only for
some resources you have an actual proper
tracking of the memory that would seem
like the ideal thing to me yeah and if I
remember correctly rust actually started
the other way around the earliest
versions of rust I think did have a
garbage collector in there baked into
the core system and over time they got
removed and garbage collection became a
library that you could add on one thing
to say about all of this is this is in
some sense Cutting Edge stuff and there
is a big design space and I think you
know we as a
community of people working on
programming languages are just starting
to explore it yeah and there are some
ways around it in the language but
having it properly supported inside o
will be need well thank you for joining
me that was a really fun and
unexpectedly wide ranging conversation
thank you for having
me you can find links to some of the
things that we talked about including
some of laurance Open Source work as
well as a full transcript of the episode
along with a glossery at signals andth
threads.com thanks for joining us and
see you next week